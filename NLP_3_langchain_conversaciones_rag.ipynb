{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gforconi/UTNIA2025/blob/main/NLP_3_langchain_conversaciones_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fe6774",
      "metadata": {
        "id": "66fe6774"
      },
      "source": [
        "\n",
        "# LangChain paso a paso: conversaci√≥n, memoria y RAG\n",
        "\n",
        "Este notebook muestra una **evoluci√≥n progresiva** del uso de LangChain en tres etapas:\n",
        "\n",
        "1. **Conversaci√≥n simple**: un ejemplo b√°sico con un LLM.\n",
        "2. **Conversaci√≥n con memoria (BufferMemory)**: extendemos el ejemplo para mantener historial.\n",
        "3. **Conversaci√≥n con RAG**: cargamos documentos, creamos un √≠ndice vectorial (Chroma) y consultamos con **conversational RAG**.\n",
        "\n",
        "> **Requisitos**: Python 3.10+ y una clave de API de OpenAI en la variable de entorno `OPENAI_API_KEY`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304baff3",
      "metadata": {
        "id": "304baff3"
      },
      "source": [
        "## 0) Instalaci√≥n y configuraci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f64b922",
      "metadata": {
        "id": "5f64b922"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ‚ú≥Ô∏è Instala dependencias (ejecuta esta celda)\n",
        "# Sugerencia: usa un entorno virtual (venv/conda) antes de instalar.\n",
        "!pip install -U langchain langchain-openai langchain-community langchain-text-splitters chromadb pypdf tiktoken\n",
        "# Opcional: FAISS en lugar de Chroma\n",
        "# !pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4dab15",
      "metadata": {
        "id": "9e4dab15",
        "outputId": "bc58c7f8-965f-4f97-a6c3-6424a6611102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OPENAI_API_KEY detectada.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# üîê Configura tu clave de OpenAI\n",
        "import os\n",
        "\n",
        "# Opci√≥n A (recomendada): exporta la variable en tu sistema antes de abrir el notebook:\n",
        "#   Linux/Mac: export OPENAI_API_KEY=\"tu_api_key\"\n",
        "#   Windows (PowerShell): setx OPENAI_API_KEY \"tu_api_key\"\n",
        "#\n",
        "# Opci√≥n B: establece la clave directamente aqu√≠ (no recomendado para producci√≥n):\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key\"\n",
        "\n",
        "# Opci√≥n B\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key_aqui\"\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ OPENAI_API_KEY en el entorno. Config√∫rala antes de continuar.\")\n",
        "else:\n",
        "    print(\"‚úÖ OPENAI_API_KEY detectada.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe046c5",
      "metadata": {
        "id": "4fe046c5"
      },
      "source": [
        "\n",
        "## 1) Conversaci√≥n simple con LangChain\n",
        "\n",
        "En esta primera secci√≥n creamos una mini **pipeline** con:\n",
        "- `ChatPromptTemplate` ‚Üí define el prompt.\n",
        "- `ChatOpenAI` ‚Üí el modelo de chat.\n",
        "- `StrOutputParser` ‚Üí convierte la respuesta a `str` para imprimir f√°cilmente.\n",
        "\n",
        "La **LCEL (LangChain Expression Language)** permite conectar componentes con el operador `|`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2460310f",
      "metadata": {
        "id": "2460310f",
        "outputId": "210d2765-16d9-4416-e7be-c5f5d9d97ca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, soy un asistente virtual dise√±ado para ayudarte con informaci√≥n y responder a tus preguntas. Puedo ofrecerte datos, consejos, explicaciones sobre diversos temas y m√°s. ¬øEn qu√© puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Modelo de chat (elige el que prefieras; gpt-4o-mini es r√°pido y econ√≥mico)\n",
        "# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Prompt de sistema + entrada del usuario\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente √∫til y conciso. Responde en espa√±ol.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Construimos la cadena: prompt -> modelo -> parser de string\n",
        "simple_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# üëá Prueba r√°pida\n",
        "respuesta = simple_chain.invoke({\"input\": \"Hola, ¬øqui√©n eres y qu√© puedes hacer?\"})\n",
        "print(respuesta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4e7bc0",
      "metadata": {
        "id": "2a4e7bc0"
      },
      "source": [
        "\n",
        "**Qu√© observar**  \n",
        "- `invoke(...)` ejecuta la cadena de principio a fin.  \n",
        "- Cambia `temperature` para controlar la creatividad.  \n",
        "- Modifica el texto de `system` para ajustar el \"rol\" del asistente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd60868d",
      "metadata": {
        "id": "dd60868d"
      },
      "source": [
        "### Temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdf633b6",
      "metadata": {
        "id": "bdf633b6"
      },
      "source": [
        "Con `temperature = 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "450c8e7a",
      "metadata": {
        "id": "450c8e7a",
        "outputId": "61af072c-98a9-4ab4-cf8f-fd627bb7aa39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo informaci√≥n en tiempo real, pero puedo ayudarte con conceptos, temas y preguntas sobre la clase de Inteligencia Artificial. ¬øQu√© aspecto espec√≠fico te gustar√≠a conocer?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6608729c",
      "metadata": {
        "id": "6608729c",
        "outputId": "d257d505-32ad-4aad-eefb-b6eaaf2e73ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a informaci√≥n en tiempo real, pero generalmente las clases de Inteligencia Artificial se centran en temas como el aprendizaje autom√°tico, redes neuronales, procesamiento de lenguaje natural y √©tica en IA. Si quieres saber sobre un tema espec√≠fico, dime y con gusto te ayudo.\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c806b41c",
      "metadata": {
        "id": "c806b41c"
      },
      "source": [
        "Con `temperature = 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e57257",
      "metadata": {
        "id": "63e57257",
        "outputId": "8aabf94a-7880-4836-ce42-929e06d0ad14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a informaci√≥n en tiempo real, pero generalmente las clases de Inteligencia Artificial suelen ser muy interesantes y abarcan temas como aprendizaje autom√°tico, redes neuronales y procesamiento de lenguaje natural. ¬øTe gustar√≠a saber algo espec√≠fico sobre el tema?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "910ac6b0",
      "metadata": {
        "id": "910ac6b0",
        "outputId": "51e55f8d-1866-4bae-bbe7-246ac9ffbf2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a informaci√≥n en tiempo real, pero generalmente las clases de Inteligencia Artificial suelen ser muy interesantes y abarcan temas como aprendizaje autom√°tico, redes neuronales y procesamiento de lenguaje natural. ¬øTe gustar√≠a saber algo espec√≠fico sobre el tema?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a86c9bb",
      "metadata": {
        "id": "8a86c9bb"
      },
      "source": [
        "`temperature = 0.0`\n",
        "\n",
        "El modelo es **determinista**: casi siempre devuelve la misma respuesta para la misma entrada.\n",
        "√ötil cuando busc√°s **precisi√≥n** y consistencia (ej. generar SQL, c√≥digo, respuestas factuales).\n",
        "\n",
        "**Valores bajos (0.1 ‚Äì 0.3)**\n",
        "\n",
        "El modelo var√≠a un poco sus respuestas, pero sigue siendo bastante confiable.\n",
        "Ideal para **chatbots serios**, res√∫menes o asistencia t√©cnica.\n",
        "\n",
        "**Valores medios (0.5 ‚Äì 0.7)**\n",
        "\n",
        "Las respuestas son m√°s variadas, con sin√≥nimos, estilos distintos o explicaciones alternativas.\n",
        "√ötil en contextos de **escritura creativa** o **brainstorming**.\n",
        "\n",
        "**Valores altos (0.8 ‚Äì 1.0 o m√°s)**\n",
        "\n",
        "El modelo se vuelve **muy creativo y arriesgado**, pudiendo inventar cosas o desviarse del tema.\n",
        "Puede servir para poes√≠a, storytelling, generaci√≥n de ideas ‚Äúlocas‚Äù.\n",
        "Pero en tareas t√©cnicas, aumenta la probabilidad de errores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227881f0",
      "metadata": {
        "id": "227881f0"
      },
      "source": [
        "\n",
        "## 2) Conversaci√≥n con memoria (ConversationBufferMemory)\n",
        "\n",
        "Ahora extendemos el ejemplo para **recordar el historial** de la charla usando `ConversationBufferMemory` y `ConversationChain`.\n",
        "\n",
        "> **Nota**: `ConversationBufferMemory` est√° **deprecado** en LangChain (se mantendr√° hasta la versi√≥n 1.0). Sigue siendo √∫til para aprender; para proyectos nuevos, LangChain recomienda usar `RunnableWithMessageHistory` o enfoques basados en **LCEL** (los ver√°s en la secci√≥n 3).\n",
        "\n",
        "**Objetivo**: que el bot entienda referencias como ‚Äúeso‚Äù, ‚Äúlo anterior‚Äù, ‚Äúella/√©l‚Äù, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af70c569",
      "metadata": {
        "id": "af70c569",
        "outputId": "d464d184-a890-4321-eff6-0db3a349ab95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ck/qx4468vx3sx6bf4nt0smfg2c0000gn/T/ipykernel_86485/261983815.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n",
            "/var/folders/ck/qx4468vx3sx6bf4nt0smfg2c0000gn/T/ipykernel_86485/261983815.py:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usuario: Hola, soy Ana y me gusta Python.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hola, soy Ana y me gusta Python.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Asistente: ¬°Hola, Ana! Encantado de conocerte. Python es un lenguaje de programaci√≥n muy vers√°til y popular. ¬øQu√© es lo que m√°s te gusta de Python? ¬øEst√°s trabajando en alg√∫n proyecto en particular o aprendiendo algo nuevo?\n",
            "\n",
            "Usuario: ¬øQu√© lenguaje dije que me gustaba?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hola, soy Ana y me gusta Python.', additional_kwargs={}, response_metadata={}), AIMessage(content='¬°Hola, Ana! Encantado de conocerte. Python es un lenguaje de programaci√≥n muy vers√°til y popular. ¬øQu√© es lo que m√°s te gusta de Python? ¬øEst√°s trabajando en alg√∫n proyecto en particular o aprendiendo algo nuevo?', additional_kwargs={}, response_metadata={})]\n",
            "Human: ¬øQu√© lenguaje dije que me gustaba?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Asistente: Dijiste que te gusta Python. Es un lenguaje muy popular y vers√°til. ¬øQu√© es lo que m√°s te atrae de √©l?\n",
            "\n",
            "--- Memoria acumulada ---\n",
            "HumanMessage ‚Üí Hola, soy Ana y me gusta Python.\n",
            "AIMessage ‚Üí ¬°Hola, Ana! Encantado de conocerte. Python es un lenguaje de programaci√≥n muy vers√°til y popular. ¬øQu√© es lo que m√°s te gusta de Python? ¬øEst√°s trabajando en alg√∫n proyecto en particular o aprendiendo algo nuevo?\n",
            "HumanMessage ‚Üí ¬øQu√© lenguaje dije que me gustaba?\n",
            "AIMessage ‚Üí Dijiste que te gusta Python. Es un lenguaje muy popular y vers√°til. ¬øQu√© es lo que m√°s te atrae de √©l?\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "\n",
        "# Guardamos TODO el historial de mensajes en memoria\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# La ConversationChain une LLM + memoria\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "# üó£Ô∏è Di√°logo de ejemplo\n",
        "print(\"Usuario: Hola, soy Ana y me gusta Python.\")\n",
        "out1 = conversation.invoke(\"Hola, soy Ana y me gusta Python.\")\n",
        "print(\"Asistente:\", out1[\"response\"])\n",
        "\n",
        "print(\"\\nUsuario: ¬øQu√© lenguaje dije que me gustaba?\")\n",
        "out2 = conversation.invoke(\"¬øQu√© lenguaje dije que me gustaba?\")\n",
        "print(\"Asistente:\", out2[\"response\"])\n",
        "\n",
        "# Puedes inspeccionar el buffer de memoria\n",
        "print(\"\\n--- Memoria acumulada ---\")\n",
        "for m in memory.chat_memory.messages:\n",
        "    print(type(m).__name__, \"‚Üí\", m.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "360e5be2",
      "metadata": {
        "id": "360e5be2"
      },
      "source": [
        "\n",
        "**Qu√© observar**  \n",
        "- La memoria **no** es persistente entre ejecuciones del notebook; vive en RAM.  \n",
        "- Para memorias largas o persistentes, considera `ConversationSummaryMemory` o guardar/recargar historiales.  \n",
        "- En la secci√≥n 3 usamos la alternativa moderna con `RunnableWithMessageHistory`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6be6bb9",
      "metadata": {
        "id": "c6be6bb9"
      },
      "source": [
        "`ConversationBufferMemory` est√° deprecado en LangChain (se mantiene hasta langchain==1.0.0), as√≠ que lo muestro para aprender pero en la secci√≥n 3 uso el enfoque moderno con RunnableWithMessageHistory y history-aware retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb5b344",
      "metadata": {
        "id": "8eb5b344"
      },
      "source": [
        "\n",
        "## 3) Conversaci√≥n con RAG (consultas a una base vectorial)\n",
        "\n",
        "En esta secci√≥n implementamos un **RAG conversacional**:\n",
        "1. Cargamos documentos locales (carpeta `./data`).\n",
        "2. Los partimos en fragmentos.\n",
        "3. Creamos embeddings y un **√≠ndice vectorial** con **Chroma**.\n",
        "4. Usamos un **retriever** que busca los pasajes m√°s relevantes.\n",
        "5. Armamos una cadena **history-aware** (consciente del historial) para contextualizar preguntas de seguimiento.\n",
        "6. Mantenemos el historial con `RunnableWithMessageHistory`.\n",
        "\n",
        "> Si no tienes documentos, crearemos **autom√°ticamente** algunos de ejemplo en `./data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e647e5",
      "metadata": {
        "id": "03e647e5",
        "outputId": "010f8ce1-4d3a-43a6-bb5c-a9b0a23f2363"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 3077.26it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 4691.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos cargados: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.1) Carga de documentos\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Si la carpeta est√° vac√≠a, creamos documentos de ejemplo\n",
        "if not any(data_dir.iterdir()):\n",
        "    (data_dir / \"intro_rag.txt\").write_text(\n",
        "        \"RAG (Retrieval-Augmented Generation) combina recuperaci√≥n de contexto con generaci√≥n. \"\n",
        "        \"Permite responder con informaci√≥n actualizada sin reentrenar el modelo.\"\n",
        "    )\n",
        "    (data_dir / \"langchain_nociones.md\").write_text(\n",
        "        \"# Nociones de LangChain\\n\"\n",
        "        \"- LCEL permite encadenar componentes con el operador |.\\n\"\n",
        "        \"- Los retrievers devuelven documentos relevantes.\\n\"\n",
        "        \"- Los vector stores como Chroma y FAISS almacenan embeddings.\"\n",
        "    )\n",
        "\n",
        "# Cargamos .txt y .md con TextLoader, y .pdf con PyPDFLoader\n",
        "loaders = []\n",
        "# Text/Markdown\n",
        "loaders.append(DirectoryLoader(str(data_dir), glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True))\n",
        "loaders.append(DirectoryLoader(str(data_dir), glob=\"**/*.md\", loader_cls=TextLoader, show_progress=True))\n",
        "# PDF\n",
        "for pdf in data_dir.glob(\"**/*.pdf\"):\n",
        "    loaders.append(PyPDFLoader(str(pdf)))\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "print(f\"Documentos cargados: {len(docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4565317",
      "metadata": {
        "id": "a4565317",
        "outputId": "c65bc6f4-d1e9-4b9d-9bca-18b40234358c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks creados: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.2) Particionado de documentos en chunks\n",
        "# En versiones recientes, los 'splitters' residen en 'langchain_text_splitters'\n",
        "try:\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "except ImportError:\n",
        "    # Fallback si usas una versi√≥n anterior\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800, # Cada fragmento tendr√° como m√°ximo 800 caracteres.\n",
        "    chunk_overlap=120, # Cada fragmento se solapa con el siguiente en 120 caracteres.\n",
        "    add_start_index=True # A√±ade el √≠ndice de inicio del chunk en el documento original (√∫til para referencias).\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Chunks creados: {len(splits)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1af6332",
      "metadata": {
        "id": "b1af6332"
      },
      "source": [
        "`chunk_size=800`\n",
        "\n",
        "Cada fragmento tendr√° como m√°ximo 800 caracteres.\n",
        "Esto ayuda a que el LLM no reciba textos demasiado largos en una sola pasada.\n",
        "\n",
        "`chunk_overlap=120`\n",
        "\n",
        "Cada fragmento se solapa con el siguiente en 120 caracteres.\n",
        "Sirve para que no se ‚Äúcorte‚Äù informaci√≥n importante justo en el l√≠mite entre dos chunks.\n",
        "Ejemplo: si un p√°rrafo empieza al final de un chunk, el solapamiento asegura que tambi√©n aparezca al inicio del siguiente.\n",
        "\n",
        "`add_start_index=True`\n",
        "\n",
        "Agrega un √≠ndice de inicio (posici√≥n dentro del texto original) en la metadata de cada fragmento.\n",
        "Eso es √∫til si despu√©s quer√©s rastrear en qu√© parte exacta del documento estaba el chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989da1a4",
      "metadata": {
        "id": "989da1a4",
        "outputId": "f40ba090-d5ca-4968-b3d3-9f379183933c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store listo ‚úì\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.3) Embeddings + Vector Store (Chroma)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "persist_dir = \"chroma_db\"\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_dir  # para que puedas reusar el √≠ndice\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "print(\"Vector store listo ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a76575",
      "metadata": {
        "id": "b6a76575",
        "outputId": "b6c762a7-c891-455a-ad4f-3df7a7f1c4c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cadenas de RAG construidas ‚úì\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.4) Cadena de RAG con historial (enfoque recomendado)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
        "\n",
        "# Prompt que reescribe la consulta considerando el historial (si lo hay)\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Reescribe la consulta del usuario para b√∫squeda, teniendo en cuenta el historial.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    prompt=contextualize_q_prompt\n",
        ")\n",
        "\n",
        "# Prompt final para responder con los documentos recuperados\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Eres un asistente experto. Usa EXCLUSIVAMENTE el 'contexto' proporcionado para responder en espa√±ol. \"\n",
        "     \"Si la respuesta no est√° en el contexto, di claramente que no aparece.\\n\\n\"\n",
        "     \"===== CONTEXTO =====\\n{context}\\n=====================\\n\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, combine_docs_chain)\n",
        "\n",
        "print(\"Cadenas de RAG construidas ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480fe48a",
      "metadata": {
        "id": "480fe48a",
        "outputId": "7e117c50-ea18-447f-88cc-621bb1f9ba4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pregunta 1: ¬øQu√© es RAG?\n",
            "‚Üí RAG (Retrieval-Augmented Generation) combina recuperaci√≥n de contexto con generaci√≥n. Permite responder con informaci√≥n actualizada sin reentrenar el modelo.\n",
            "\n",
            "Pregunta 2: ¬øY c√≥mo se relaciona con LangChain? (nota: usa un pronombre)\n",
            "‚Üí No aparece.\n",
            "\n",
            "Pregunta 3: ¬øD√≥nde mencionaste los retrievers?\n",
            "‚Üí Los retrievers se mencionan en el contexto como aquellos que devuelven documentos relevantes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.5) A√±adir memoria de conversaci√≥n con RunnableWithMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory  # almacenamiento simple en memoria\n",
        "\n",
        "# Diccionario {session_id: ChatMessageHistory}\n",
        "_session_store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in _session_store:\n",
        "        _session_store[session_id] = ChatMessageHistory()\n",
        "    return _session_store[session_id]\n",
        "\n",
        "rag_with_history = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",  # la clave por defecto para create_retrieval_chain\n",
        ")\n",
        "\n",
        "# Demostraci√≥n de di√°logo multi-turno con el mismo session_id\n",
        "config = {\"configurable\": {\"session_id\": \"demo\"}}\n",
        "\n",
        "def conversar(pregunta: str):\n",
        "    resp = rag_with_history.invoke({\"input\": pregunta}, config=config)\n",
        "    print(\"‚Üí\", resp[\"answer\"])\n",
        "    # Si quieres inspeccionar qu√© documentos us√≥:\n",
        "    # for i, d in enumerate(resp.get(\"context\", []), 1):\n",
        "    #     print(f\"[{i}] {d.metadata.get('source', 'doc')}:{d.metadata.get('page', '')}\")\n",
        "\n",
        "print(\"Pregunta 1: ¬øQu√© es RAG?\")\n",
        "conversar(\"¬øQu√© es RAG?\")\n",
        "\n",
        "print(\"\\nPregunta 2: ¬øY c√≥mo se relaciona con LangChain? (nota: usa un pronombre)\")\n",
        "conversar(\"¬øY c√≥mo se relaciona con LangChain?\")\n",
        "\n",
        "print(\"\\nPregunta 3: ¬øD√≥nde mencionaste los retrievers?\")\n",
        "conversar(\"¬øD√≥nde mencionaste los retrievers?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36068732",
      "metadata": {
        "id": "36068732"
      },
      "source": [
        "### Consultas la base de vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5206fcb",
      "metadata": {
        "id": "a5206fcb",
        "outputId": "d1e469e5-afdb-4bfa-9338-e2149a2470b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8152427673339844 data/intro_rag.txt\n",
            "1.3764668703079224 data/langchain_nociones.md\n"
          ]
        }
      ],
      "source": [
        "query = \"¬øQu√© es RAG?\"\n",
        "pairs = vectordb.similarity_search_with_score(query, k=4)\n",
        "for doc, score in pairs:\n",
        "    print(score, doc.metadata.get(\"source\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bd742f",
      "metadata": {
        "id": "70bd742f",
        "outputId": "93167d91-2360-490d-d847-db97933e9743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] data/intro_rag.txt  p√°g: None\n",
            "RAG (Retrieval-Augmented Generation) combina recuperaci√≥n de contexto con generaci√≥n. Permite responder con informaci√≥n actualizada sin reentrenar el modelo. ...\n",
            "\n",
            "[2] data/langchain_nociones.md  p√°g: None\n",
            "# Nociones de LangChain\n",
            "- LCEL permite encadenar componentes con el operador |.\n",
            "- Los retrievers devuelven documentos relevantes.\n",
            "- Los vector stores como Chroma y FAISS almacenan embeddings. ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"¬øQu√© es RAG?\"\n",
        "docs = retriever.invoke(query)  # equivalente a retriever.get_relevant_documents(query)\n",
        "\n",
        "for i, d in enumerate(docs, 1):\n",
        "    print(f\"[{i}] {d.metadata.get('source', 'doc')}  p√°g: {d.metadata.get('page')}\")\n",
        "    print(d.page_content[:300], \"...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3774931e",
      "metadata": {
        "id": "3774931e"
      },
      "source": [
        "\n",
        "### Notas y buenas pr√°cticas\n",
        "- Si ves advertencias de *deprecations*, revisa las gu√≠as de migraci√≥n de LangChain y actualiza imports.\n",
        "- Para **persistir** historiales entre sesiones, puedes reemplazar `ChatMessageHistory` por alternativas como `FileChatMessageHistory` o una base de datos.\n",
        "- **FAISS** es otra base vectorial popular (`faiss-cpu`). El c√≥digo cambia poco: `from langchain_community.vectorstores import FAISS` y `FAISS.from_documents(...)`.\n",
        "- Ajusta `chunk_size`/`chunk_overlap` seg√∫n el tama√±o de tus documentos y el contexto del modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1e6f9d",
      "metadata": {
        "id": "ae1e6f9d"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Cr√©ditos y recursos\n",
        "- Documentaci√≥n de **ChatOpenAI** y `langchain-openai` (instalaci√≥n y uso).\n",
        "- `ConversationBufferMemory` (estado: deprecado, pero √∫til para aprender).\n",
        "- RAG moderno con `create_history_aware_retriever` y `create_retrieval_chain`.\n",
        "- `create_stuff_documents_chain` para combinar documentos en el prompt.\n",
        "\n",
        "> Revisa las gu√≠as oficiales para ejemplos actualizados, cambios en APIs y mejores pr√°cticas.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}