{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gforconi/UTNIA2025/blob/main/NLP_Conceptos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c922bb6",
      "metadata": {
        "id": "1c922bb6"
      },
      "source": [
        "\n",
        "# Clase de NLP — Demo práctica\n",
        "**Contenidos:** Tokenización y lematización (NLTK), clasificación de sentimientos con TF‑IDF + Naive Bayes (scikit‑learn), **mini‑RAG** (búsqueda + generación simple) y **agente** de juguete con herramientas.\n",
        "\n",
        "> Ejecutá las celdas en orden. Si es la primera vez, corré la celda de *instalación rápida*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407b13f3",
      "metadata": {
        "id": "407b13f3"
      },
      "source": [
        "## 🚀 Instalación rápida (si te falta algo)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK (Natural Language Toolkit)**\n",
        "\n",
        "Es una librería de Python para Procesamiento de Lenguaje Natural (PLN).\n",
        "\n",
        "Proporciona herramientas para:\n",
        "\n",
        "- Tokenizar (separar palabras y oraciones).\n",
        "\n",
        "- Eliminar stopwords (palabras sin valor semántico como “el”, “de”).\n",
        "\n",
        "- Stemming y lematización (reducir palabras a su raíz).\n",
        "\n",
        "- Trabajar con corpus (ejemplos) de texto.\n",
        "\n",
        "Se usa mucho en investigación, enseñanza y prototipos de análisis de texto."
      ],
      "metadata": {
        "id": "d4wZAyBYNeND"
      },
      "id": "d4wZAyBYNeND"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**scikit-learn**\n",
        "\n",
        "Permite:\n",
        "\n",
        "- Clasificación, regresión, clustering.\n",
        "\n",
        "- Preprocesamiento de datos (vectorización de texto, escalado, normalización).\n",
        "\n",
        "- Modelos estadísticos y algoritmos de ML (Naive Bayes, SVM, árboles, etc.).\n",
        "\n",
        "- Muy usada para crear y entrenar modelos predictivos de forma rápida y sencilla."
      ],
      "metadata": {
        "id": "_Ujd-wtqN4Bs"
      },
      "id": "_Ujd-wtqN4Bs"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "528aaa3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "528aaa3a",
        "outputId": "3c1766da-681a-4166-ceec-d0c528696089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listo ✅\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejecutá esta celda si no tenés las librerías instaladas.\n",
        "# (Podés volver a ejecutarla sin problemas)\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkg):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg])\n",
        "\n",
        "for pkg in [\"nltk\", \"scikit-learn\"]:\n",
        "    try:\n",
        "        __import__(pkg.split(\"==\")[0])\n",
        "    except Exception:\n",
        "        pip_install(pkg)\n",
        "\n",
        "import nltk\n",
        "# Descargar recursos necesarios para tokenización y lematización\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"Listo ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652c2b3f",
      "metadata": {
        "id": "652c2b3f"
      },
      "source": [
        "## 1) Tokenización y lematización (NLTK)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizar:** Es el proceso de dividir un texto en unidades más pequeñas llamadas tokens.\n",
        "\n",
        "Un token puede ser una palabra, un número, un signo de puntuación o incluso una oración, según cómo lo definas.\n",
        "\n",
        "Se pueden tokenizar palabras, oraciones, parrafos.\n",
        "\n",
        "**Lematizar:** Es reducir una palabra a su forma base o “lema” (la que encontrarías en un diccionario). Tiene en cuenta la gramática y el contexto.\n",
        "\n",
        "**Stemming:** es una técnica que recorta las palabras a su raíz (stem), sin importar si la raíz es una palabra válida en el idioma. Se basa en reglas simples de cortar sufijos y prefijos.\n",
        "\n",
        "El stem puede no ser una palabra válida, pero sirve para agrupar variantes parecidas.\n",
        "\n",
        "**Lematizar vs Stemming:**\n",
        "\n",
        "Lematizar es más avanzado que un stemming, ya que el stemming solo corta palabras).\n",
        "\n",
        "Stemming es más mecánico y agresivo que la lematización.\n"
      ],
      "metadata": {
        "id": "ugL6SOkdOcLM"
      },
      "id": "ugL6SOkdOcLM"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9521e7ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9521e7ac",
        "outputId": "ab74b8da-666a-430f-f7e7-630f10126901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['los', 'modelos', 'de', 'lenguaje', 'grandes', 'están', 'transformando', 'el', 'nlp', 'rápidamente', '.']\n",
            "Lemmas: ['los', 'modelos', 'de', 'lenguaje', 'grandes', 'están', 'transformando', 'el', 'nlp', 'rápidamente', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "texto = \"Los modelos de lenguaje grandes están transformando el NLP rápidamente.\"\n",
        "tokens = word_tokenize(texto.lower(), language='spanish')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Lemmas:\", lemmas)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The cats are running faster than the dogs\"\n",
        "\n",
        "# Tokenización\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Lematización\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "print(\"Lemmas:\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNz4L0vIKeKp",
        "outputId": "06cd5901-207b-4c6c-f778-799867fe7b02"
      },
      "id": "mNz4L0vIKeKp",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'cats', 'are', 'running', 'faster', 'than', 'the', 'dogs']\n",
            "Lemmas: ['The', 'cat', 'are', 'running', 'faster', 'than', 'the', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"cats\" → \"cat\"\n",
        "\n",
        "\"dogs\" → \"dog\"\n",
        "\n",
        "\"running\" no cambia porque la lematización de NLTK necesita la parte de la oración (POS tag) para hacerlo bien."
      ],
      "metadata": {
        "id": "PVxKFV-lKwI7"
      },
      "id": "PVxKFV-lKwI7"
    },
    {
      "cell_type": "markdown",
      "id": "8d07071f",
      "metadata": {
        "id": "8d07071f"
      },
      "source": [
        "## 2) Clasificación de sentimientos con TF‑IDF + Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las principales librerias que vamos a utilizar son:\n",
        "\n",
        "**TfidfVectorizer:** transforma el texto en vectores numéricos usando la técnica TF-IDF (Term Frequency – Inverse Document Frequency).\n",
        "→ Sirve para representar qué tan importantes son las palabras en cada oración.\n",
        "\n",
        "**MultinomialNB:** clasificador Naive Bayes Multinomial, muy usado para texto porque funciona bien con conteos/frecuencias de palabras.\n",
        "\n",
        "**make_pipeline:** crea un pipeline que encadena varias transformaciones y un modelo en una sola estructura."
      ],
      "metadata": {
        "id": "3wTndSTSM59x"
      },
      "id": "3wTndSTSM59x"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dcf6af89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcf6af89",
        "outputId": "66334cb0-4aeb-465e-d5b6-c48e093f4b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'muy bueno y recomendable' -> pos\n",
            "'esto es una estafa' -> neg\n",
            "'calidad normal' -> pos\n",
            "'no sirve' -> neg\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Mini dataset de ejemplo (español)\n",
        "# Frases\n",
        "X = [\n",
        "    \"Me encanta este producto, funciona de maravilla\",\n",
        "    \"Horrible experiencia, no lo recomiendo a nadie\",\n",
        "    \"Excelente atención al cliente, muy satisfecho\",\n",
        "    \"Malo y defectuoso, perdí mi dinero\",\n",
        "    \"Buen desempeño y calidad aceptable\",\n",
        "    \"Terrible, se rompe a los dos días\",\n",
        "    \"Fantástico, superó mis expectativas\",\n",
        "    \"Pésimo servicio y respuesta lenta\"\n",
        "]\n",
        "# Etiqueta de cada frase\n",
        "y = [\"pos\", \"neg\", \"pos\", \"neg\", \"pos\", \"neg\", \"pos\", \"neg\"]\n",
        "\n",
        "# Creacion del modelo:\n",
        "# Aquí el pipeline hace dos cosas:\n",
        "# 1. TfidfVectorizer\n",
        "#     ngram_range=(1,2) → usa tanto palabras individuales (unigramas) como pares de palabras consecutivas (bigramas).\n",
        "#     min_df=1 → incluye términos que aparecen al menos en 1 documento.\n",
        "# 2. MultinomialNB → clasifica el texto transformado en TF-IDF como pos o neg.\n",
        "\n",
        "modelo = make_pipeline(TfidfVectorizer(ngram_range=(1,2), min_df=1), MultinomialNB())\n",
        "\n",
        "# Entrenamiento\n",
        "modelo.fit(X, y)\n",
        "\n",
        "pruebas = [\"muy bueno y recomendable\", \"esto es una estafa\", \"calidad normal\", \"no sirve\"]\n",
        "pred = modelo.predict(pruebas)\n",
        "\n",
        "for t, p in zip(pruebas, pred):\n",
        "    print(f\"{t!r} -> {p}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En resumen:\n",
        "\n",
        "Este script es un clasificador de sentimientos muy básico en español.\n",
        "\n",
        "- Usa TF-IDF para convertir texto en números.\n",
        "\n",
        "- Usa Naive Bayes para clasificar en positivo o negativo.\n",
        "\n",
        "- Demuestra cómo entrenar y luego usar el modelo para predecir frases nuevas."
      ],
      "metadata": {
        "id": "w_Y8C7YrQe6r"
      },
      "id": "w_Y8C7YrQe6r"
    },
    {
      "cell_type": "markdown",
      "id": "ad2791e5",
      "metadata": {
        "id": "ad2791e5"
      },
      "source": [
        "### Mirar características TF‑IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a3c8db5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a3c8db5",
        "outputId": "ae46b2b2-72e6-4449-97ea-ac6df6ad2124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top POS: ['buen desempeño' 'calidad' 'desempeño calidad' 'expectativas' 'aceptable'\n",
            " 'buen' 'desempeño' 'calidad aceptable' 'superó mis' 'superó']\n",
            "Top NEG: ['mi dinero' 'perdí mi' 'perdí' 'pésimo servicio' 'pésimo' 'lenta'\n",
            " 'respuesta' 'servicio respuesta' 'respuesta lenta' 'servicio']\n"
          ]
        }
      ],
      "source": [
        "vect = modelo.named_steps['tfidfvectorizer']\n",
        "nb = modelo.named_steps['multinomialnb']\n",
        "\n",
        "feature_names = vect.get_feature_names_out()\n",
        "import numpy as np\n",
        "\n",
        "# Top características más influyentes para 'pos' y 'neg'\n",
        "class_idx_pos = list(nb.classes_).index('pos')\n",
        "top_pos = np.argsort(nb.feature_log_prob_[class_idx_pos])[-10:]\n",
        "class_idx_neg = list(nb.classes_).index('neg')\n",
        "top_neg = np.argsort(nb.feature_log_prob_[class_idx_neg])[-10:]\n",
        "\n",
        "print(\"Top POS:\", feature_names[top_pos])\n",
        "print(\"Top NEG:\", feature_names[top_neg])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f073b7f",
      "metadata": {
        "id": "4f073b7f"
      },
      "source": [
        "## 3) Mini‑RAG (búsqueda + generación simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1d82c6",
      "metadata": {
        "id": "cc1d82c6"
      },
      "source": [
        "\n",
        "**Idea:** guardamos documentos cortos, buscamos los más relevantes con TF‑IDF y **generamos** una respuesta tomando frases de esos documentos.  \n",
        "> No requiere API externas ni claves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "09dfcef8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09dfcef8",
        "outputId": "314a4e20-6eda-44bc-f4dd-873a690a213e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pasajes recuperados:\n",
            "- D2 (score=0.302): RAG combina recuperación de información con modelos generativos. Primero busca pasajes relevantes y luego el LLM genera la respuesta.\n",
            "- D4 (score=0.212): Un agente puede decidir qué herramienta usar (calculadora, búsqueda, API) antes de generar una respuesta final.\n",
            "\n",
            "Respuesta:\n",
            "RAG combina recuperación de información con modelos generativos. Primero busca pasajes relevantes y luego el LLM genera la respuesta. (Respuesta generada a partir de pasajes recuperados).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "docs = [\n",
        "    \"El NLP permite que las computadoras entiendan y generen lenguaje humano. Involucra tareas como clasificación, NER y traducción.\",\n",
        "    \"RAG combina recuperación de información con modelos generativos. Primero busca pasajes relevantes y luego el LLM genera la respuesta.\",\n",
        "    \"Los embeddings o TF-IDF pueden representar documentos para búsqueda semántica. Luego se seleccionan los top-k más similares.\",\n",
        "    \"Un agente puede decidir qué herramienta usar (calculadora, búsqueda, API) antes de generar una respuesta final.\"\n",
        "]\n",
        "\n",
        "doc_ids = [f\"D{i+1}\" for i in range(len(docs))]\n",
        "\n",
        "# Indexado\n",
        "vectorizer = TfidfVectorizer(stop_words=None)\n",
        "M = vectorizer.fit_transform(docs)\n",
        "\n",
        "def retrieve(query, k=2):\n",
        "    qv = vectorizer.transform([query])\n",
        "    sims = cosine_similarity(qv, M).ravel()\n",
        "    idx = sims.argsort()[::-1][:k]\n",
        "    return [(doc_ids[i], docs[i], float(sims[i])) for i in idx]\n",
        "\n",
        "def simple_generate(query, retrieved):\n",
        "    # Generación muy simple: concatena oraciones relevantes y agrega un cierre.\n",
        "    context = \" \".join([r[1] for r in retrieved])\n",
        "    # Selección de frases que contienen palabras clave\n",
        "    import re\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', context)\n",
        "    keywords = [w for w in query.lower().split() if len(w) > 3]\n",
        "    chosen = [s for s in sentences if any(k in s.lower() for k in keywords)]\n",
        "    if not chosen:\n",
        "        chosen = sentences[:2]\n",
        "    answer = \" \".join(chosen)\n",
        "    return answer + \" (Respuesta generada a partir de pasajes recuperados).\"\n",
        "\n",
        "query = \"¿Qué es RAG y cómo se usa con un LLM?\"\n",
        "retrieved = retrieve(query, k=2)\n",
        "print(\"Pasajes recuperados:\")\n",
        "for rid, text, score in retrieved:\n",
        "    print(f\"- {rid} (score={score:.3f}): {text}\")\n",
        "\n",
        "print(\"\\nRespuesta:\")\n",
        "print(simple_generate(query, retrieved))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f896efba",
      "metadata": {
        "id": "f896efba"
      },
      "source": [
        "## 4) Agente de juguete (herramientas + traza)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4162ebd",
      "metadata": {
        "id": "d4162ebd"
      },
      "source": [
        "\n",
        "**Flujo:** El \"agente\" decide si usar la **calculadora** o la **búsqueda** (mini‑RAG) según la consulta.  \n",
        "Mostramos una **traza** de decisiones para explicar qué hizo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e48a5d6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e48a5d6f",
        "outputId": "0b448a6f-78cb-4e7f-f212-5738ad8d7489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta: Calcula 12 * (3 + 4)\n",
            "Traza:\n",
            " - Decisión: usar calculadora\n",
            " - Calculadora ->  12 * (3 + 4) = 84\n",
            "Respuesta: El resultado es 84.\n",
            "------------------------------------------------------------\n",
            "Consulta: ¿Qué es un agente en NLP?\n",
            "Traza:\n",
            " - Decisión: usar búsqueda (mini‑RAG)\n",
            " - Recuperados: ['D4', 'D1']\n",
            "Respuesta: Un agente puede decidir qué herramienta usar (calculadora, búsqueda, API) antes de generar una respuesta final. (Respuesta generada a partir de pasajes recuperados).\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import operator\n",
        "import re\n",
        "\n",
        "def tool_calculator(expr: str):\n",
        "    # Seguridad básica: solo números, espacio y operadores + - * / ( )\n",
        "    if not re.fullmatch(r\"[0-9+\\-*/().\\s]+\", expr):\n",
        "        raise ValueError(\"Expresión no permitida\")\n",
        "    return eval(expr)\n",
        "\n",
        "def tool_search(query: str):\n",
        "    return retrieve(query, k=2)\n",
        "\n",
        "def agent(query: str):\n",
        "    trace = []\n",
        "    # Decidir herramienta\n",
        "    if re.search(r\"[0-9]\\s*[+\\-*/]\", query) or any(w in query.lower() for w in [\"calcula\", \"sumá\", \"multiplicá\"]):\n",
        "        trace.append(\"Decisión: usar calculadora\")\n",
        "        expr = re.findall(r\"[0-9+\\-*/().\\s]+\", query)\n",
        "        expr = expr[0] if expr else query\n",
        "        result = tool_calculator(expr)\n",
        "        trace.append(f\"Calculadora -> {expr} = {result}\")\n",
        "        answer = f\"El resultado es {result}.\"\n",
        "    else:\n",
        "        trace.append(\"Decisión: usar búsqueda (mini‑RAG)\")\n",
        "        hits = tool_search(query)\n",
        "        trace.append(f\"Recuperados: {[h[0] for h in hits]}\")\n",
        "        answer = simple_generate(query, hits)\n",
        "    return trace, answer\n",
        "\n",
        "# Pruebas\n",
        "for q in [\"Calcula 12 * (3 + 4)\", \"¿Qué es un agente en NLP?\" ]:\n",
        "    t, a = agent(q)\n",
        "    print(\"Consulta:\", q)\n",
        "    print(\"Traza:\")\n",
        "    for step in t:\n",
        "        print(\" -\", step)\n",
        "    print(\"Respuesta:\", a)\n",
        "    print(\"-\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ba7377",
      "metadata": {
        "id": "c7ba7377"
      },
      "source": [
        "\n",
        "---\n",
        "### ✅ ¿Qué aprendimos?\n",
        "- Cómo **preprocesar** texto (tokenizar/lematizar).\n",
        "- Entrenar un **clasificador** simple con TF‑IDF + Naive Bayes.\n",
        "- Armar un **mini‑RAG** sin dependencias externas.\n",
        "- Implementar un **agente** de juguete que decide herramientas y expone su traza.\n",
        "\n",
        "> Para llevarlo al siguiente nivel, vamos a cambiar TF‑IDF por **embeddings** y conectar un LLM real. Ayudados por el framework [LangChain](https://www.langchain.com/)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}