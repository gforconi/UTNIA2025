{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gforconi/UTNIA2025/blob/main/NLP_Conceptos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c922bb6",
      "metadata": {
        "id": "1c922bb6"
      },
      "source": [
        "\n",
        "# Clase de NLP ‚Äî Demo pr√°ctica\n",
        "**Contenidos:** Tokenizaci√≥n y lematizaci√≥n (NLTK), clasificaci√≥n de sentimientos con TF‚ÄëIDF + Naive Bayes (scikit‚Äëlearn), **mini‚ÄëRAG** (b√∫squeda + generaci√≥n simple) y **agente** de juguete con herramientas.\n",
        "\n",
        "> Ejecut√° las celdas en orden. Si es la primera vez, corr√© la celda de *instalaci√≥n r√°pida*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407b13f3",
      "metadata": {
        "id": "407b13f3"
      },
      "source": [
        "## üöÄ Instalaci√≥n r√°pida (si te falta algo)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK (Natural Language Toolkit)**\n",
        "\n",
        "Es una librer√≠a de Python para Procesamiento de Lenguaje Natural (PLN).\n",
        "\n",
        "Proporciona herramientas para:\n",
        "\n",
        "- Tokenizar (separar palabras y oraciones).\n",
        "\n",
        "- Eliminar stopwords (palabras sin valor sem√°ntico como ‚Äúel‚Äù, ‚Äúde‚Äù).\n",
        "\n",
        "- Stemming y lematizaci√≥n (reducir palabras a su ra√≠z).\n",
        "\n",
        "- Trabajar con corpus (ejemplos) de texto.\n",
        "\n",
        "Se usa mucho en investigaci√≥n, ense√±anza y prototipos de an√°lisis de texto."
      ],
      "metadata": {
        "id": "d4wZAyBYNeND"
      },
      "id": "d4wZAyBYNeND"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**scikit-learn**\n",
        "\n",
        "Permite:\n",
        "\n",
        "- Clasificaci√≥n, regresi√≥n, clustering.\n",
        "\n",
        "- Preprocesamiento de datos (vectorizaci√≥n de texto, escalado, normalizaci√≥n).\n",
        "\n",
        "- Modelos estad√≠sticos y algoritmos de ML (Naive Bayes, SVM, √°rboles, etc.).\n",
        "\n",
        "- Muy usada para crear y entrenar modelos predictivos de forma r√°pida y sencilla."
      ],
      "metadata": {
        "id": "_Ujd-wtqN4Bs"
      },
      "id": "_Ujd-wtqN4Bs"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "528aaa3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "528aaa3a",
        "outputId": "3c1766da-681a-4166-ceec-d0c528696089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listo ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejecut√° esta celda si no ten√©s las librer√≠as instaladas.\n",
        "# (Pod√©s volver a ejecutarla sin problemas)\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkg):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg])\n",
        "\n",
        "for pkg in [\"nltk\", \"scikit-learn\"]:\n",
        "    try:\n",
        "        __import__(pkg.split(\"==\")[0])\n",
        "    except Exception:\n",
        "        pip_install(pkg)\n",
        "\n",
        "import nltk\n",
        "# Descargar recursos necesarios para tokenizaci√≥n y lematizaci√≥n\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"Listo ‚úÖ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652c2b3f",
      "metadata": {
        "id": "652c2b3f"
      },
      "source": [
        "## 1) Tokenizaci√≥n y lematizaci√≥n (NLTK)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizar:** Es el proceso de dividir un texto en unidades m√°s peque√±as llamadas tokens.\n",
        "\n",
        "Un token puede ser una palabra, un n√∫mero, un signo de puntuaci√≥n o incluso una oraci√≥n, seg√∫n c√≥mo lo definas.\n",
        "\n",
        "Se pueden tokenizar palabras, oraciones, parrafos.\n",
        "\n",
        "**Lematizar:** Es reducir una palabra a su forma base o ‚Äúlema‚Äù (la que encontrar√≠as en un diccionario). Tiene en cuenta la gram√°tica y el contexto.\n",
        "\n",
        "**Stemming:** es una t√©cnica que recorta las palabras a su ra√≠z (stem), sin importar si la ra√≠z es una palabra v√°lida en el idioma. Se basa en reglas simples de cortar sufijos y prefijos.\n",
        "\n",
        "El stem puede no ser una palabra v√°lida, pero sirve para agrupar variantes parecidas.\n",
        "\n",
        "**Lematizar vs Stemming:**\n",
        "\n",
        "Lematizar es m√°s avanzado que un stemming, ya que el stemming solo corta palabras).\n",
        "\n",
        "Stemming es m√°s mec√°nico y agresivo que la lematizaci√≥n.\n"
      ],
      "metadata": {
        "id": "ugL6SOkdOcLM"
      },
      "id": "ugL6SOkdOcLM"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9521e7ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9521e7ac",
        "outputId": "ab74b8da-666a-430f-f7e7-630f10126901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['los', 'modelos', 'de', 'lenguaje', 'grandes', 'est√°n', 'transformando', 'el', 'nlp', 'r√°pidamente', '.']\n",
            "Lemmas: ['los', 'modelos', 'de', 'lenguaje', 'grandes', 'est√°n', 'transformando', 'el', 'nlp', 'r√°pidamente', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "texto = \"Los modelos de lenguaje grandes est√°n transformando el NLP r√°pidamente.\"\n",
        "tokens = word_tokenize(texto.lower(), language='spanish')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Lemmas:\", lemmas)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The cats are running faster than the dogs\"\n",
        "\n",
        "# Tokenizaci√≥n\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Lematizaci√≥n\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "print(\"Lemmas:\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNz4L0vIKeKp",
        "outputId": "06cd5901-207b-4c6c-f778-799867fe7b02"
      },
      "id": "mNz4L0vIKeKp",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'cats', 'are', 'running', 'faster', 'than', 'the', 'dogs']\n",
            "Lemmas: ['The', 'cat', 'are', 'running', 'faster', 'than', 'the', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"cats\" ‚Üí \"cat\"\n",
        "\n",
        "\"dogs\" ‚Üí \"dog\"\n",
        "\n",
        "\"running\" no cambia porque la lematizaci√≥n de NLTK necesita la parte de la oraci√≥n (POS tag) para hacerlo bien."
      ],
      "metadata": {
        "id": "PVxKFV-lKwI7"
      },
      "id": "PVxKFV-lKwI7"
    },
    {
      "cell_type": "markdown",
      "id": "8d07071f",
      "metadata": {
        "id": "8d07071f"
      },
      "source": [
        "## 2) Clasificaci√≥n de sentimientos con TF‚ÄëIDF + Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las principales librerias que vamos a utilizar son:\n",
        "\n",
        "**TfidfVectorizer:** transforma el texto en vectores num√©ricos usando la t√©cnica TF-IDF (Term Frequency ‚Äì Inverse Document Frequency).\n",
        "‚Üí Sirve para representar qu√© tan importantes son las palabras en cada oraci√≥n.\n",
        "\n",
        "**MultinomialNB:** clasificador Naive Bayes Multinomial, muy usado para texto porque funciona bien con conteos/frecuencias de palabras.\n",
        "\n",
        "**make_pipeline:** crea un pipeline que encadena varias transformaciones y un modelo en una sola estructura."
      ],
      "metadata": {
        "id": "3wTndSTSM59x"
      },
      "id": "3wTndSTSM59x"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dcf6af89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcf6af89",
        "outputId": "66334cb0-4aeb-465e-d5b6-c48e093f4b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'muy bueno y recomendable' -> pos\n",
            "'esto es una estafa' -> neg\n",
            "'calidad normal' -> pos\n",
            "'no sirve' -> neg\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Mini dataset de ejemplo (espa√±ol)\n",
        "# Frases\n",
        "X = [\n",
        "    \"Me encanta este producto, funciona de maravilla\",\n",
        "    \"Horrible experiencia, no lo recomiendo a nadie\",\n",
        "    \"Excelente atenci√≥n al cliente, muy satisfecho\",\n",
        "    \"Malo y defectuoso, perd√≠ mi dinero\",\n",
        "    \"Buen desempe√±o y calidad aceptable\",\n",
        "    \"Terrible, se rompe a los dos d√≠as\",\n",
        "    \"Fant√°stico, super√≥ mis expectativas\",\n",
        "    \"P√©simo servicio y respuesta lenta\"\n",
        "]\n",
        "# Etiqueta de cada frase\n",
        "y = [\"pos\", \"neg\", \"pos\", \"neg\", \"pos\", \"neg\", \"pos\", \"neg\"]\n",
        "\n",
        "# Creacion del modelo:\n",
        "# Aqu√≠ el pipeline hace dos cosas:\n",
        "# 1. TfidfVectorizer\n",
        "#     ngram_range=(1,2) ‚Üí usa tanto palabras individuales (unigramas) como pares de palabras consecutivas (bigramas).\n",
        "#     min_df=1 ‚Üí incluye t√©rminos que aparecen al menos en 1 documento.\n",
        "# 2. MultinomialNB ‚Üí clasifica el texto transformado en TF-IDF como pos o neg.\n",
        "\n",
        "modelo = make_pipeline(TfidfVectorizer(ngram_range=(1,2), min_df=1), MultinomialNB())\n",
        "\n",
        "# Entrenamiento\n",
        "modelo.fit(X, y)\n",
        "\n",
        "pruebas = [\"muy bueno y recomendable\", \"esto es una estafa\", \"calidad normal\", \"no sirve\"]\n",
        "pred = modelo.predict(pruebas)\n",
        "\n",
        "for t, p in zip(pruebas, pred):\n",
        "    print(f\"{t!r} -> {p}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En resumen:\n",
        "\n",
        "Este script es un clasificador de sentimientos muy b√°sico en espa√±ol.\n",
        "\n",
        "- Usa TF-IDF para convertir texto en n√∫meros.\n",
        "\n",
        "- Usa Naive Bayes para clasificar en positivo o negativo.\n",
        "\n",
        "- Demuestra c√≥mo entrenar y luego usar el modelo para predecir frases nuevas."
      ],
      "metadata": {
        "id": "w_Y8C7YrQe6r"
      },
      "id": "w_Y8C7YrQe6r"
    },
    {
      "cell_type": "markdown",
      "id": "ad2791e5",
      "metadata": {
        "id": "ad2791e5"
      },
      "source": [
        "### Mirar caracter√≠sticas TF‚ÄëIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a3c8db5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a3c8db5",
        "outputId": "ae46b2b2-72e6-4449-97ea-ac6df6ad2124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top POS: ['buen desempe√±o' 'calidad' 'desempe√±o calidad' 'expectativas' 'aceptable'\n",
            " 'buen' 'desempe√±o' 'calidad aceptable' 'super√≥ mis' 'super√≥']\n",
            "Top NEG: ['mi dinero' 'perd√≠ mi' 'perd√≠' 'p√©simo servicio' 'p√©simo' 'lenta'\n",
            " 'respuesta' 'servicio respuesta' 'respuesta lenta' 'servicio']\n"
          ]
        }
      ],
      "source": [
        "vect = modelo.named_steps['tfidfvectorizer']\n",
        "nb = modelo.named_steps['multinomialnb']\n",
        "\n",
        "feature_names = vect.get_feature_names_out()\n",
        "import numpy as np\n",
        "\n",
        "# Top caracter√≠sticas m√°s influyentes para 'pos' y 'neg'\n",
        "class_idx_pos = list(nb.classes_).index('pos')\n",
        "top_pos = np.argsort(nb.feature_log_prob_[class_idx_pos])[-10:]\n",
        "class_idx_neg = list(nb.classes_).index('neg')\n",
        "top_neg = np.argsort(nb.feature_log_prob_[class_idx_neg])[-10:]\n",
        "\n",
        "print(\"Top POS:\", feature_names[top_pos])\n",
        "print(\"Top NEG:\", feature_names[top_neg])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f073b7f",
      "metadata": {
        "id": "4f073b7f"
      },
      "source": [
        "## 3) Mini‚ÄëRAG (b√∫squeda + generaci√≥n simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1d82c6",
      "metadata": {
        "id": "cc1d82c6"
      },
      "source": [
        "\n",
        "**Idea:** guardamos documentos cortos, buscamos los m√°s relevantes con TF‚ÄëIDF y **generamos** una respuesta tomando frases de esos documentos.  \n",
        "> No requiere API externas ni claves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "09dfcef8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09dfcef8",
        "outputId": "314a4e20-6eda-44bc-f4dd-873a690a213e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pasajes recuperados:\n",
            "- D2 (score=0.302): RAG combina recuperaci√≥n de informaci√≥n con modelos generativos. Primero busca pasajes relevantes y luego el LLM genera la respuesta.\n",
            "- D4 (score=0.212): Un agente puede decidir qu√© herramienta usar (calculadora, b√∫squeda, API) antes de generar una respuesta final.\n",
            "\n",
            "Respuesta:\n",
            "RAG combina recuperaci√≥n de informaci√≥n con modelos generativos. Primero busca pasajes relevantes y luego el LLM genera la respuesta. (Respuesta generada a partir de pasajes recuperados).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "docs = [\n",
        "    \"El NLP permite que las computadoras entiendan y generen lenguaje humano. Involucra tareas como clasificaci√≥n, NER y traducci√≥n.\",\n",
        "    \"RAG combina recuperaci√≥n de informaci√≥n con modelos generativos. Primero busca pasajes relevantes y luego el LLM genera la respuesta.\",\n",
        "    \"Los embeddings o TF-IDF pueden representar documentos para b√∫squeda sem√°ntica. Luego se seleccionan los top-k m√°s similares.\",\n",
        "    \"Un agente puede decidir qu√© herramienta usar (calculadora, b√∫squeda, API) antes de generar una respuesta final.\"\n",
        "]\n",
        "\n",
        "doc_ids = [f\"D{i+1}\" for i in range(len(docs))]\n",
        "\n",
        "# Indexado\n",
        "vectorizer = TfidfVectorizer(stop_words=None)\n",
        "M = vectorizer.fit_transform(docs)\n",
        "\n",
        "def retrieve(query, k=2):\n",
        "    qv = vectorizer.transform([query])\n",
        "    sims = cosine_similarity(qv, M).ravel()\n",
        "    idx = sims.argsort()[::-1][:k]\n",
        "    return [(doc_ids[i], docs[i], float(sims[i])) for i in idx]\n",
        "\n",
        "def simple_generate(query, retrieved):\n",
        "    # Generaci√≥n muy simple: concatena oraciones relevantes y agrega un cierre.\n",
        "    context = \" \".join([r[1] for r in retrieved])\n",
        "    # Selecci√≥n de frases que contienen palabras clave\n",
        "    import re\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', context)\n",
        "    keywords = [w for w in query.lower().split() if len(w) > 3]\n",
        "    chosen = [s for s in sentences if any(k in s.lower() for k in keywords)]\n",
        "    if not chosen:\n",
        "        chosen = sentences[:2]\n",
        "    answer = \" \".join(chosen)\n",
        "    return answer + \" (Respuesta generada a partir de pasajes recuperados).\"\n",
        "\n",
        "query = \"¬øQu√© es RAG y c√≥mo se usa con un LLM?\"\n",
        "retrieved = retrieve(query, k=2)\n",
        "print(\"Pasajes recuperados:\")\n",
        "for rid, text, score in retrieved:\n",
        "    print(f\"- {rid} (score={score:.3f}): {text}\")\n",
        "\n",
        "print(\"\\nRespuesta:\")\n",
        "print(simple_generate(query, retrieved))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f896efba",
      "metadata": {
        "id": "f896efba"
      },
      "source": [
        "## 4) Agente de juguete (herramientas + traza)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4162ebd",
      "metadata": {
        "id": "d4162ebd"
      },
      "source": [
        "\n",
        "**Flujo:** El \"agente\" decide si usar la **calculadora** o la **b√∫squeda** (mini‚ÄëRAG) seg√∫n la consulta.  \n",
        "Mostramos una **traza** de decisiones para explicar qu√© hizo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e48a5d6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e48a5d6f",
        "outputId": "0b448a6f-78cb-4e7f-f212-5738ad8d7489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta: Calcula 12 * (3 + 4)\n",
            "Traza:\n",
            " - Decisi√≥n: usar calculadora\n",
            " - Calculadora ->  12 * (3 + 4) = 84\n",
            "Respuesta: El resultado es 84.\n",
            "------------------------------------------------------------\n",
            "Consulta: ¬øQu√© es un agente en NLP?\n",
            "Traza:\n",
            " - Decisi√≥n: usar b√∫squeda (mini‚ÄëRAG)\n",
            " - Recuperados: ['D4', 'D1']\n",
            "Respuesta: Un agente puede decidir qu√© herramienta usar (calculadora, b√∫squeda, API) antes de generar una respuesta final. (Respuesta generada a partir de pasajes recuperados).\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import operator\n",
        "import re\n",
        "\n",
        "def tool_calculator(expr: str):\n",
        "    # Seguridad b√°sica: solo n√∫meros, espacio y operadores + - * / ( )\n",
        "    if not re.fullmatch(r\"[0-9+\\-*/().\\s]+\", expr):\n",
        "        raise ValueError(\"Expresi√≥n no permitida\")\n",
        "    return eval(expr)\n",
        "\n",
        "def tool_search(query: str):\n",
        "    return retrieve(query, k=2)\n",
        "\n",
        "def agent(query: str):\n",
        "    trace = []\n",
        "    # Decidir herramienta\n",
        "    if re.search(r\"[0-9]\\s*[+\\-*/]\", query) or any(w in query.lower() for w in [\"calcula\", \"sum√°\", \"multiplic√°\"]):\n",
        "        trace.append(\"Decisi√≥n: usar calculadora\")\n",
        "        expr = re.findall(r\"[0-9+\\-*/().\\s]+\", query)\n",
        "        expr = expr[0] if expr else query\n",
        "        result = tool_calculator(expr)\n",
        "        trace.append(f\"Calculadora -> {expr} = {result}\")\n",
        "        answer = f\"El resultado es {result}.\"\n",
        "    else:\n",
        "        trace.append(\"Decisi√≥n: usar b√∫squeda (mini‚ÄëRAG)\")\n",
        "        hits = tool_search(query)\n",
        "        trace.append(f\"Recuperados: {[h[0] for h in hits]}\")\n",
        "        answer = simple_generate(query, hits)\n",
        "    return trace, answer\n",
        "\n",
        "# Pruebas\n",
        "for q in [\"Calcula 12 * (3 + 4)\", \"¬øQu√© es un agente en NLP?\" ]:\n",
        "    t, a = agent(q)\n",
        "    print(\"Consulta:\", q)\n",
        "    print(\"Traza:\")\n",
        "    for step in t:\n",
        "        print(\" -\", step)\n",
        "    print(\"Respuesta:\", a)\n",
        "    print(\"-\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ba7377",
      "metadata": {
        "id": "c7ba7377"
      },
      "source": [
        "\n",
        "---\n",
        "### ‚úÖ ¬øQu√© aprendimos?\n",
        "- C√≥mo **preprocesar** texto (tokenizar/lematizar).\n",
        "- Entrenar un **clasificador** simple con TF‚ÄëIDF + Naive Bayes.\n",
        "- Armar un **mini‚ÄëRAG** sin dependencias externas.\n",
        "- Implementar un **agente** de juguete que decide herramientas y expone su traza.\n",
        "\n",
        "> Para llevarlo al siguiente nivel, vamos a cambiar TF‚ÄëIDF por **embeddings** y conectar un LLM real. Ayudados por el framework [LangChain](https://www.langchain.com/)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}