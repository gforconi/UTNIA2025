{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gforconi/UTNIA2025/blob/main/NLP_3_langchain_conversaciones_rag_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fe6774",
      "metadata": {
        "id": "66fe6774"
      },
      "source": [
        "\n",
        "# LangChain paso a paso: conversaci√≥n, memoria y RAG\n",
        "\n",
        "Este notebook muestra una **evoluci√≥n progresiva** del uso de LangChain en tres etapas:\n",
        "\n",
        "1. **Conversaci√≥n simple**: un ejemplo b√°sico con un LLM.\n",
        "2. **Conversaci√≥n con memoria (BufferMemory)**: extendemos el ejemplo para mantener historial.\n",
        "3. **Conversaci√≥n con RAG**: cargamos documentos, creamos un √≠ndice vectorial (Chroma) y consultamos con **conversational RAG**.\n",
        "\n",
        "> **Requisitos**: Python 3.10+ y una clave de API de OpenAI en la variable de entorno `OPENAI_API_KEY`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304baff3",
      "metadata": {
        "id": "304baff3"
      },
      "source": [
        "## 0) Instalaci√≥n y configuraci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b0855c",
      "metadata": {
        "id": "a5b0855c"
      },
      "source": [
        "\n",
        "### (Opcional) Usar modelo local con Ollama en lugar de OpenAI\n",
        "Si quer√©s correr **local**, activ√° `USE_OLLAMA = True` y asegurate de tener `ollama` corriendo en `http://localhost:11434`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15ada22",
      "metadata": {
        "id": "a15ada22"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Selector de proveedor LLM: OpenAI (default) u Ollama (local)\n",
        "USE_OLLAMA = False  # ‚Üê cambi√° a True para usar Ollama local\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "try:\n",
        "    if USE_OLLAMA:\n",
        "        from langchain_ollama import ChatOllama\n",
        "        llm_factory = lambda: ChatOllama(model=\"llama3\", temperature=0.1)\n",
        "    else:\n",
        "        llm_factory = lambda: ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
        "    print(\"Proveedor listo:\", \"Ollama\" if USE_OLLAMA else \"OpenAI\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Error al configurar el proveedor LLM. ¬øInstalaste 'langchain-ollama' si USE_OLLAMA=True?\") from e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f64b922",
      "metadata": {
        "id": "5f64b922",
        "outputId": "4cbf6839-bc26-45d6-990c-94d4da810193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting langsmith>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.4.23-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
            "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.13.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
            "  Downloading openai-1.104.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from tiktoken) (2025.7.34)\n",
            "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting sniffio (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>4 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
            "  Using cached pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Using cached httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain-community) (1.26.4)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (73 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
            "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.7 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-macosx_13_0_universal2.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from chromadb) (0.21.4)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb)\n",
            "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting importlib-resources (from chromadb)\n",
            "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting grpcio>=1.58.0 (from chromadb)\n",
            "  Downloading grpcio-1.74.0-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
            "Collecting typer>=0.9.0 (from chromadb)\n",
            "  Downloading typer-0.17.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (14 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.11.3-cp312-cp312-macosx_15_0_arm64.whl.metadata (41 kB)\n",
            "Collecting rich>=10.11.0 (from chromadb)\n",
            "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting jsonschema>=4.19.0 (from chromadb)\n",
            "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
            "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
            "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb)\n",
            "  Downloading rpds_py-0.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
            "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.24.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.1 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: sympy in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: filelock in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.8)\n",
            "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
            "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
            "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
            "Downloading sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
            "Using cached tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (996 kB)\n",
            "Downloading openai-1.104.2-py3-none-any.whl (928 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m928.2/928.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Using cached jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl (320 kB)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl (469 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
            "Using cached pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl (89 kB)\n",
            "Downloading chromadb-1.0.20-cp39-abi3-macosx_11_0_arm64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
            "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl (46 kB)\n",
            "Downloading grpcio-1.74.0-cp312-cp312-macosx_11_0_universal2.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
            "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading langsmith-0.4.23-py3-none-any.whl (378 kB)\n",
            "Downloading mmh3-5.2.0-cp312-cp312-macosx_11_0_arm64.whl (40 kB)\n",
            "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "Downloading onnxruntime-1.22.1-cp312-cp312-macosx_13_0_universal2.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
            "Downloading orjson-3.11.3-cp312-cp312-macosx_15_0_arm64.whl (127 kB)\n",
            "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
            "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-macosx_11_0_arm64.whl (31 kB)\n",
            "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
            "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading rpds_py-0.27.1-cp312-cp312-macosx_11_0_arm64.whl (345 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading typer-0.17.3-py3-none-any.whl (46 kB)\n",
            "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-macosx_11_0_arm64.whl (104 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-macosx_10_13_universal2.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-macosx_11_0_arm64.whl (394 kB)\n",
            "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading zstandard-0.24.0-cp312-cp312-macosx_11_0_arm64.whl (640 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m640.3/640.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=a71b282885ea84974469df0be3a7d2fc8a5979e6eaab0d931bba7050f777ea3a\n",
            "  Stored in directory: /Users/geronimoforconi/Library/Caches/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, flatbuffers, durationpy, zstandard, zipp, websockets, websocket-client, uvloop, typing-inspection, tenacity, SQLAlchemy, sniffio, shellingham, rpds-py, requests, python-dotenv, pyproject_hooks, pypdf, pydantic-core, pybase64, pyasn1, protobuf, propcache, overrides, orjson, oauthlib, mypy-extensions, multidict, mmh3, mdurl, marshmallow, jsonpointer, jiter, importlib-resources, humanfriendly, httpx-sse, httptools, h11, grpcio, frozenlist, distro, click, cachetools, bcrypt, backoff, attrs, annotated-types, aiohappyeyeballs, yarl, uvicorn, typing-inspect, tiktoken, rsa, requests-toolbelt, requests-oauthlib, referencing, pydantic, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, jsonpatch, importlib-metadata, httpcore, googleapis-common-protos, coloredlogs, build, anyio, aiosignal, watchfiles, rich, pydantic-settings, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, jsonschema-specifications, httpx, google-auth, dataclasses-json, aiohttp, typer, opentelemetry-semantic-conventions, openai, langsmith, kubernetes, jsonschema, opentelemetry-sdk, langchain-core, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, langchain, chromadb, langchain-community\n",
            "\u001b[2K  Attempting uninstall: requests‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10/94\u001b[0m [SQLAlchemy]lient]\n",
            "\u001b[2K    Found existing installation: requests 2.32.4‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10/94\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K    Uninstalling requests-2.32.4:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10/94\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K      Successfully uninstalled requests-2.32.4‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10/94\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94/94\u001b[0m [langchain-community]ommunity]ore]dk]protos]\n",
            "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.10.0 attrs-25.3.0 backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 cachetools-5.5.2 chromadb-1.0.20 click-8.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 distro-1.9.0 durationpy-0.10 flatbuffers-25.2.10 frozenlist-1.7.0 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.74.0 h11-0.16.0 httpcore-1.0.9 httptools-0.6.4 httpx-0.28.1 httpx-sse-0.4.1 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 kubernetes-33.1.0 langchain-0.3.27 langchain-community-0.3.29 langchain-core-0.3.75 langchain-openai-0.3.32 langchain-text-splitters-0.3.11 langsmith-0.4.23 markdown-it-py-4.0.0 marshmallow-3.26.1 mdurl-0.1.2 mmh3-5.2.0 multidict-6.6.4 mypy-extensions-1.1.0 oauthlib-3.3.1 onnxruntime-1.22.1 openai-1.104.2 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 orjson-3.11.3 overrides-7.7.0 posthog-5.4.0 propcache-0.3.2 protobuf-6.32.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 pypdf-6.0.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.1 referencing-0.36.2 requests-2.32.5 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-14.1.0 rpds-py-0.27.1 rsa-4.9.1 shellingham-1.5.4 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.11.0 typer-0.17.3 typing-inspect-0.9.0 typing-inspection-0.4.1 uvicorn-0.35.0 uvloop-0.21.0 watchfiles-1.1.0 websocket-client-1.8.0 websockets-15.0.1 yarl-1.20.1 zipp-3.23.0 zstandard-0.24.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚ú≥Ô∏è Instala dependencias (ejecuta esta celda)\n",
        "# Sugerencia: usa un entorno virtual (venv/conda) antes de instalar.\n",
        "!pip install -U langchain langchain-openai langchain-community langchain-text-splitters chromadb pypdf tiktoken\n",
        "# Opcional: FAISS en lugar de Chroma\n",
        "# !pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4dab15",
      "metadata": {
        "id": "9e4dab15",
        "outputId": "356577d4-005d-4d2e-81a7-2ce55b395643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OPENAI_API_KEY detectada.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# üîê Configura tu clave de OpenAI\n",
        "import os\n",
        "\n",
        "# Opci√≥n A (recomendada): exporta la variable en tu sistema antes de abrir el notebook:\n",
        "#   Linux/Mac: export OPENAI_API_KEY=\"tu_api_key\"\n",
        "#   Windows (PowerShell): setx OPENAI_API_KEY \"tu_api_key\"\n",
        "#\n",
        "# Opci√≥n B: establece la clave directamente aqu√≠ (no recomendado para producci√≥n):\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key\"\n",
        "\n",
        "# Opci√≥n B\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key_aqui\"\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ OPENAI_API_KEY en el entorno. Config√∫rala antes de continuar.\")\n",
        "else:\n",
        "    print(\"‚úÖ OPENAI_API_KEY detectada.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe046c5",
      "metadata": {
        "id": "4fe046c5"
      },
      "source": [
        "\n",
        "## 1) Conversaci√≥n simple con LangChain\n",
        "\n",
        "En esta primera secci√≥n creamos una mini **pipeline** con:\n",
        "- `ChatPromptTemplate` ‚Üí define el prompt.\n",
        "- `ChatOpenAI` ‚Üí el modelo de chat.\n",
        "- `StrOutputParser` ‚Üí convierte la respuesta a `str` para imprimir f√°cilmente.\n",
        "\n",
        "La **LCEL (LangChain Expression Language)** permite conectar componentes con el operador `|`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2460310f",
      "metadata": {
        "id": "2460310f",
        "outputId": "4283019c-f1ff-4536-ead7-76ae6419cc59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, soy un asistente virtual dise√±ado para ayudarte con informaci√≥n y responder a tus preguntas. Puedo ofrecerte datos, consejos, explicaciones sobre diversos temas y m√°s. ¬øEn qu√© puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Modelo de chat (elige el que prefieras; gpt-4o-mini es r√°pido y econ√≥mico)\n",
        "# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Prompt de sistema + entrada del usuario\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente √∫til y conciso. Responde en espa√±ol.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Construimos la cadena: prompt -> modelo -> parser de string\n",
        "simple_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# üëá Prueba r√°pida\n",
        "respuesta = simple_chain.invoke({\"input\": \"Hola, ¬øqui√©n eres y qu√© puedes hacer?\"})\n",
        "print(respuesta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4e7bc0",
      "metadata": {
        "id": "2a4e7bc0"
      },
      "source": [
        "\n",
        "**Qu√© observar**  \n",
        "- `invoke(...)` ejecuta la cadena de principio a fin.  \n",
        "- Cambia `temperature` para controlar la creatividad.  \n",
        "- Modifica el texto de `system` para ajustar el \"rol\" del asistente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd60868d",
      "metadata": {
        "id": "dd60868d"
      },
      "source": [
        "### Temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdf633b6",
      "metadata": {
        "id": "bdf633b6"
      },
      "source": [
        "Con `temperature = 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "450c8e7a",
      "metadata": {
        "id": "450c8e7a",
        "outputId": "228f73a8-6b30-4c64-87ce-3e7c754f2c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo informaci√≥n en tiempo real, pero puedo ayudarte con conceptos, temas y preguntas sobre la clase de Inteligencia Artificial. ¬øQu√© aspecto espec√≠fico te gustar√≠a conocer?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6608729c",
      "metadata": {
        "id": "6608729c",
        "outputId": "ecc437c6-7de5-4fa7-e635-7c2ba3218bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a informaci√≥n en tiempo real, pero generalmente las clases de Inteligencia Artificial se centran en temas como el aprendizaje autom√°tico, redes neuronales, procesamiento de lenguaje natural y √©tica en IA. Si quieres saber sobre un tema espec√≠fico, dime y con gusto te ayudo.\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c806b41c",
      "metadata": {
        "id": "c806b41c"
      },
      "source": [
        "Con `temperature = 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e57257",
      "metadata": {
        "id": "63e57257",
        "outputId": "08285a54-7a24-4040-a627-001e9fc14e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a informaci√≥n en tiempo real, pero generalmente las clases de Inteligencia Artificial suelen ser muy interesantes y abarcan temas como aprendizaje autom√°tico, redes neuronales y procesamiento de lenguaje natural. ¬øTe gustar√≠a saber algo espec√≠fico sobre el tema?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "910ac6b0",
      "metadata": {
        "id": "910ac6b0",
        "outputId": "2d795915-4346-4835-eca5-78c6c2046a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a informaci√≥n en tiempo real, pero generalmente las clases de Inteligencia Artificial suelen ser muy interesantes y abarcan temas como aprendizaje autom√°tico, redes neuronales y procesamiento de lenguaje natural. ¬øTe gustar√≠a saber algo espec√≠fico sobre el tema?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a86c9bb",
      "metadata": {
        "id": "8a86c9bb"
      },
      "source": [
        "`temperature = 0.0`\n",
        "\n",
        "El modelo es **determinista**: casi siempre devuelve la misma respuesta para la misma entrada.\n",
        "√ötil cuando busc√°s **precisi√≥n** y consistencia (ej. generar SQL, c√≥digo, respuestas factuales).\n",
        "\n",
        "**Valores bajos (0.1 ‚Äì 0.3)**\n",
        "\n",
        "El modelo var√≠a un poco sus respuestas, pero sigue siendo bastante confiable.\n",
        "Ideal para **chatbots serios**, res√∫menes o asistencia t√©cnica.\n",
        "\n",
        "**Valores medios (0.5 ‚Äì 0.7)**\n",
        "\n",
        "Las respuestas son m√°s variadas, con sin√≥nimos, estilos distintos o explicaciones alternativas.\n",
        "√ötil en contextos de **escritura creativa** o **brainstorming**.\n",
        "\n",
        "**Valores altos (0.8 ‚Äì 1.0 o m√°s)**\n",
        "\n",
        "El modelo se vuelve **muy creativo y arriesgado**, pudiendo inventar cosas o desviarse del tema.\n",
        "Puede servir para poes√≠a, storytelling, generaci√≥n de ideas ‚Äúlocas‚Äù.\n",
        "Pero en tareas t√©cnicas, aumenta la probabilidad de errores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227881f0",
      "metadata": {
        "id": "227881f0"
      },
      "source": [
        "\n",
        "## 2) Conversaci√≥n con memoria (ConversationBufferMemory)\n",
        "\n",
        "Ahora extendemos el ejemplo para **recordar el historial** de la charla usando `ConversationBufferMemory` y `ConversationChain`.\n",
        "\n",
        "> **Nota**: `ConversationBufferMemory` est√° **deprecado** en LangChain (se mantendr√° hasta la versi√≥n 1.0). Sigue siendo √∫til para aprender; para proyectos nuevos, LangChain recomienda usar `RunnableWithMessageHistory` o enfoques basados en **LCEL** (los ver√°s en la secci√≥n 3).\n",
        "\n",
        "**Objetivo**: que el bot entienda referencias como ‚Äúeso‚Äù, ‚Äúlo anterior‚Äù, ‚Äúella/√©l‚Äù, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af70c569",
      "metadata": {
        "id": "af70c569",
        "outputId": "a51fb383-7711-4ee0-80c5-f070068b7b99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ck/qx4468vx3sx6bf4nt0smfg2c0000gn/T/ipykernel_86485/261983815.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n",
            "/var/folders/ck/qx4468vx3sx6bf4nt0smfg2c0000gn/T/ipykernel_86485/261983815.py:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usuario: Hola, soy Ana y me gusta Python.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hola, soy Ana y me gusta Python.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Asistente: ¬°Hola, Ana! Encantado de conocerte. Python es un lenguaje de programaci√≥n muy vers√°til y popular. ¬øQu√© es lo que m√°s te gusta de Python? ¬øEst√°s trabajando en alg√∫n proyecto en particular o aprendiendo algo nuevo?\n",
            "\n",
            "Usuario: ¬øQu√© lenguaje dije que me gustaba?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hola, soy Ana y me gusta Python.', additional_kwargs={}, response_metadata={}), AIMessage(content='¬°Hola, Ana! Encantado de conocerte. Python es un lenguaje de programaci√≥n muy vers√°til y popular. ¬øQu√© es lo que m√°s te gusta de Python? ¬øEst√°s trabajando en alg√∫n proyecto en particular o aprendiendo algo nuevo?', additional_kwargs={}, response_metadata={})]\n",
            "Human: ¬øQu√© lenguaje dije que me gustaba?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Asistente: Dijiste que te gusta Python. Es un lenguaje muy popular y vers√°til. ¬øQu√© es lo que m√°s te atrae de √©l?\n",
            "\n",
            "--- Memoria acumulada ---\n",
            "HumanMessage ‚Üí Hola, soy Ana y me gusta Python.\n",
            "AIMessage ‚Üí ¬°Hola, Ana! Encantado de conocerte. Python es un lenguaje de programaci√≥n muy vers√°til y popular. ¬øQu√© es lo que m√°s te gusta de Python? ¬øEst√°s trabajando en alg√∫n proyecto en particular o aprendiendo algo nuevo?\n",
            "HumanMessage ‚Üí ¬øQu√© lenguaje dije que me gustaba?\n",
            "AIMessage ‚Üí Dijiste que te gusta Python. Es un lenguaje muy popular y vers√°til. ¬øQu√© es lo que m√°s te atrae de √©l?\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "\n",
        "# Guardamos TODO el historial de mensajes en memoria\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# La ConversationChain une LLM + memoria\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "# üó£Ô∏è Di√°logo de ejemplo\n",
        "print(\"Usuario: Hola, soy Ana y me gusta Python.\")\n",
        "out1 = conversation.invoke(\"Hola, soy Ana y me gusta Python.\")\n",
        "print(\"Asistente:\", out1[\"response\"])\n",
        "\n",
        "print(\"\\nUsuario: ¬øQu√© lenguaje dije que me gustaba?\")\n",
        "out2 = conversation.invoke(\"¬øQu√© lenguaje dije que me gustaba?\")\n",
        "print(\"Asistente:\", out2[\"response\"])\n",
        "\n",
        "# Puedes inspeccionar el buffer de memoria\n",
        "print(\"\\n--- Memoria acumulada ---\")\n",
        "for m in memory.chat_memory.messages:\n",
        "    print(type(m).__name__, \"‚Üí\", m.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "360e5be2",
      "metadata": {
        "id": "360e5be2"
      },
      "source": [
        "\n",
        "**Qu√© observar**  \n",
        "- La memoria **no** es persistente entre ejecuciones del notebook; vive en RAM.  \n",
        "- Para memorias largas o persistentes, considera `ConversationSummaryMemory` o guardar/recargar historiales.  \n",
        "- En la secci√≥n 3 usamos la alternativa moderna con `RunnableWithMessageHistory`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6be6bb9",
      "metadata": {
        "id": "c6be6bb9"
      },
      "source": [
        "`ConversationBufferMemory` est√° deprecado en LangChain (se mantiene hasta langchain==1.0.0), as√≠ que lo muestro para aprender pero en la secci√≥n 3 uso el enfoque moderno con RunnableWithMessageHistory y history-aware retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb5b344",
      "metadata": {
        "id": "8eb5b344"
      },
      "source": [
        "\n",
        "## 3) Conversaci√≥n con RAG (consultas a una base vectorial)\n",
        "\n",
        "En esta secci√≥n implementamos un **RAG conversacional**:\n",
        "1. Cargamos documentos locales (carpeta `./data`).\n",
        "2. Los partimos en fragmentos.\n",
        "3. Creamos embeddings y un **√≠ndice vectorial** con **Chroma**.\n",
        "4. Usamos un **retriever** que busca los pasajes m√°s relevantes.\n",
        "5. Armamos una cadena **history-aware** (consciente del historial) para contextualizar preguntas de seguimiento.\n",
        "6. Mantenemos el historial con `RunnableWithMessageHistory`.\n",
        "\n",
        "> Si no tienes documentos, crearemos **autom√°ticamente** algunos de ejemplo en `./data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e647e5",
      "metadata": {
        "id": "03e647e5",
        "outputId": "fc8e00d1-7c75-4036-cdfc-49bead32e66d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 3077.26it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 4691.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos cargados: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.1) Carga de documentos\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Si la carpeta est√° vac√≠a, creamos documentos de ejemplo\n",
        "if not any(data_dir.iterdir()):\n",
        "    (data_dir / \"intro_rag.txt\").write_text(\n",
        "        \"RAG (Retrieval-Augmented Generation) combina recuperaci√≥n de contexto con generaci√≥n. \"\n",
        "        \"Permite responder con informaci√≥n actualizada sin reentrenar el modelo.\"\n",
        "    )\n",
        "    (data_dir / \"langchain_nociones.md\").write_text(\n",
        "        \"# Nociones de LangChain\\n\"\n",
        "        \"- LCEL permite encadenar componentes con el operador |.\\n\"\n",
        "        \"- Los retrievers devuelven documentos relevantes.\\n\"\n",
        "        \"- Los vector stores como Chroma y FAISS almacenan embeddings.\"\n",
        "    )\n",
        "\n",
        "# Cargamos .txt y .md con TextLoader, y .pdf con PyPDFLoader\n",
        "loaders = []\n",
        "# Text/Markdown\n",
        "loaders.append(DirectoryLoader(str(data_dir), glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True))\n",
        "loaders.append(DirectoryLoader(str(data_dir), glob=\"**/*.md\", loader_cls=TextLoader, show_progress=True))\n",
        "# PDF\n",
        "for pdf in data_dir.glob(\"**/*.pdf\"):\n",
        "    loaders.append(PyPDFLoader(str(pdf)))\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "print(f\"Documentos cargados: {len(docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4565317",
      "metadata": {
        "id": "a4565317",
        "outputId": "4bedf236-441c-4110-ec29-4bcdc65d4d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks creados: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.2) Particionado de documentos en chunks\n",
        "# En versiones recientes, los 'splitters' residen en 'langchain_text_splitters'\n",
        "try:\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "except ImportError:\n",
        "    # Fallback si usas una versi√≥n anterior\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800, # Cada fragmento tendr√° como m√°ximo 800 caracteres.\n",
        "    chunk_overlap=120, # Cada fragmento se solapa con el siguiente en 120 caracteres.\n",
        "    add_start_index=True # A√±ade el √≠ndice de inicio del chunk en el documento original (√∫til para referencias).\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Chunks creados: {len(splits)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1af6332",
      "metadata": {
        "id": "b1af6332"
      },
      "source": [
        "`chunk_size=800`\n",
        "\n",
        "Cada fragmento tendr√° como m√°ximo 800 caracteres.\n",
        "Esto ayuda a que el LLM no reciba textos demasiado largos en una sola pasada.\n",
        "\n",
        "`chunk_overlap=120`\n",
        "\n",
        "Cada fragmento se solapa con el siguiente en 120 caracteres.\n",
        "Sirve para que no se ‚Äúcorte‚Äù informaci√≥n importante justo en el l√≠mite entre dos chunks.\n",
        "Ejemplo: si un p√°rrafo empieza al final de un chunk, el solapamiento asegura que tambi√©n aparezca al inicio del siguiente.\n",
        "\n",
        "`add_start_index=True`\n",
        "\n",
        "Agrega un √≠ndice de inicio (posici√≥n dentro del texto original) en la metadata de cada fragmento.\n",
        "Eso es √∫til si despu√©s quer√©s rastrear en qu√© parte exacta del documento estaba el chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989da1a4",
      "metadata": {
        "id": "989da1a4",
        "outputId": "09c4b366-28c6-4325-d247-f3f922f0480d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store listo ‚úì\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.3) Embeddings + Vector Store (Chroma)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "persist_dir = \"chroma_db\"\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_dir  # para que puedas reusar el √≠ndice\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "print(\"Vector store listo ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a76575",
      "metadata": {
        "id": "b6a76575",
        "outputId": "0b668c00-901f-4d98-cadc-44788d8be6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cadenas de RAG construidas ‚úì\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.4) Cadena de RAG con historial (enfoque recomendado)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "llm = llm_factory()\n",
        "\n",
        "# Prompt que reescribe la consulta considerando el historial (si lo hay)\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Reescribe la consulta del usuario para b√∫squeda, teniendo en cuenta el historial.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    prompt=contextualize_q_prompt\n",
        ")\n",
        "\n",
        "# Prompt final para responder con los documentos recuperados\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Eres un asistente experto. Usa EXCLUSIVAMENTE el 'contexto' proporcionado para responder en espa√±ol. \"\n",
        "     \"Si la respuesta no est√° en el contexto, di claramente que no aparece.\\n\\n\"\n",
        "     \"===== CONTEXTO =====\\n{context}\\n=====================\\n\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, combine_docs_chain)\n",
        "\n",
        "print(\"Cadenas de RAG construidas ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480fe48a",
      "metadata": {
        "id": "480fe48a",
        "outputId": "57f39d1a-0744-4dac-d5f1-dd980171c287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pregunta 1: ¬øQu√© es RAG?\n",
            "‚Üí RAG (Retrieval-Augmented Generation) combina recuperaci√≥n de contexto con generaci√≥n. Permite responder con informaci√≥n actualizada sin reentrenar el modelo.\n",
            "\n",
            "Pregunta 2: ¬øY c√≥mo se relaciona con LangChain? (nota: usa un pronombre)\n",
            "‚Üí No aparece.\n",
            "\n",
            "Pregunta 3: ¬øD√≥nde mencionaste los retrievers?\n",
            "‚Üí Los retrievers se mencionan en el contexto como aquellos que devuelven documentos relevantes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.5) A√±adir memoria de conversaci√≥n con RunnableWithMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory  # almacenamiento simple en memoria\n",
        "\n",
        "# Diccionario {session_id: ChatMessageHistory}\n",
        "_session_store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in _session_store:\n",
        "        _session_store[session_id] = ChatMessageHistory()\n",
        "    return _session_store[session_id]\n",
        "\n",
        "rag_with_history = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",  # la clave por defecto para create_retrieval_chain\n",
        ")\n",
        "\n",
        "# Demostraci√≥n de di√°logo multi-turno con el mismo session_id\n",
        "config = {\"configurable\": {\"session_id\": \"demo\"}}\n",
        "\n",
        "def conversar(pregunta: str):\n",
        "    resp = rag_with_history.invoke({\"input\": pregunta}, config=config)\n",
        "    print(\"‚Üí\", resp[\"answer\"])\n",
        "    # Si quieres inspeccionar qu√© documentos us√≥:\n",
        "    # for i, d in enumerate(resp.get(\"context\", []), 1):\n",
        "    #     print(f\"[{i}] {d.metadata.get('source', 'doc')}:{d.metadata.get('page', '')}\")\n",
        "\n",
        "print(\"Pregunta 1: ¬øQu√© es RAG?\")\n",
        "conversar(\"¬øQu√© es RAG?\")\n",
        "\n",
        "print(\"\\nPregunta 2: ¬øY c√≥mo se relaciona con LangChain? (nota: usa un pronombre)\")\n",
        "conversar(\"¬øY c√≥mo se relaciona con LangChain?\")\n",
        "\n",
        "print(\"\\nPregunta 3: ¬øD√≥nde mencionaste los retrievers?\")\n",
        "conversar(\"¬øD√≥nde mencionaste los retrievers?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36068732",
      "metadata": {
        "id": "36068732"
      },
      "source": [
        "### Consultas la base de vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5206fcb",
      "metadata": {
        "id": "a5206fcb",
        "outputId": "161bfcde-8115-44dc-f65e-f3fe05fadeab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8152427673339844 data/intro_rag.txt\n",
            "1.3764668703079224 data/langchain_nociones.md\n"
          ]
        }
      ],
      "source": [
        "query = \"¬øQu√© es RAG?\"\n",
        "pairs = vectordb.similarity_search_with_score(query, k=4)\n",
        "for doc, score in pairs:\n",
        "    print(score, doc.metadata.get(\"source\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bd742f",
      "metadata": {
        "id": "70bd742f",
        "outputId": "936423b8-222f-4007-fd01-9a1e5936cda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] data/intro_rag.txt  p√°g: None\n",
            "RAG (Retrieval-Augmented Generation) combina recuperaci√≥n de contexto con generaci√≥n. Permite responder con informaci√≥n actualizada sin reentrenar el modelo. ...\n",
            "\n",
            "[2] data/langchain_nociones.md  p√°g: None\n",
            "# Nociones de LangChain\n",
            "- LCEL permite encadenar componentes con el operador |.\n",
            "- Los retrievers devuelven documentos relevantes.\n",
            "- Los vector stores como Chroma y FAISS almacenan embeddings. ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"¬øQu√© es RAG?\"\n",
        "docs = retriever.invoke(query)  # equivalente a retriever.get_relevant_documents(query)\n",
        "\n",
        "for i, d in enumerate(docs, 1):\n",
        "    print(f\"[{i}] {d.metadata.get('source', 'doc')}  p√°g: {d.metadata.get('page')}\")\n",
        "    print(d.page_content[:300], \"...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3774931e",
      "metadata": {
        "id": "3774931e"
      },
      "source": [
        "\n",
        "### Notas y buenas pr√°cticas\n",
        "- Si ves advertencias de *deprecations*, revisa las gu√≠as de migraci√≥n de LangChain y actualiza imports.\n",
        "- Para **persistir** historiales entre sesiones, puedes reemplazar `ChatMessageHistory` por alternativas como `FileChatMessageHistory` o una base de datos.\n",
        "- **FAISS** es otra base vectorial popular (`faiss-cpu`). El c√≥digo cambia poco: `from langchain_community.vectorstores import FAISS` y `FAISS.from_documents(...)`.\n",
        "- Ajusta `chunk_size`/`chunk_overlap` seg√∫n el tama√±o de tus documentos y el contexto del modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1e6f9d",
      "metadata": {
        "id": "ae1e6f9d"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Cr√©ditos y recursos\n",
        "- Documentaci√≥n de **ChatOpenAI** y `langchain-openai` (instalaci√≥n y uso).\n",
        "- `ConversationBufferMemory` (estado: deprecado, pero √∫til para aprender).\n",
        "- RAG moderno con `create_history_aware_retriever` y `create_retrieval_chain`.\n",
        "- `create_stuff_documents_chain` para combinar documentos en el prompt.\n",
        "\n",
        "> Revisa las gu√≠as oficiales para ejemplos actualizados, cambios en APIs y mejores pr√°cticas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81b249db",
      "metadata": {
        "id": "81b249db"
      },
      "source": [
        "### Funci√≥n de ayuda: `preguntar()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "896374e7",
      "metadata": {
        "id": "896374e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preguntar(texto: str, session_id: str = \"demo\", mostrar_fuentes: bool = True):\n",
        "    if 'rag_with_history' in globals():\n",
        "        config = {\"configurable\": {\"session_id\": session_id}}\n",
        "        resp = rag_with_history.invoke({\"input\": texto}, config=config)\n",
        "    elif 'rag_chain' in globals():\n",
        "        resp = rag_chain.invoke({\"input\": texto, \"chat_history\": []})\n",
        "    else:\n",
        "        raise RuntimeError(\"No encontr√© 'rag_with_history' ni 'rag_chain'. Ejecut√° las celdas de la secci√≥n 3.4 y 3.5.\")\n",
        "\n",
        "    print(resp.get(\"answer\") or resp)\n",
        "    if mostrar_fuentes and isinstance(resp, dict) and \"context\" in resp:\n",
        "        print(\"\\nFuentes:\")\n",
        "        for i, d in enumerate(resp[\"context\"], 1):\n",
        "            src = d.metadata.get(\"source\", \"doc\")\n",
        "            page = d.metadata.get(\"page\", \"\")\n",
        "            print(f\"[{i}] {src} {('p√°g ' + str(page)) if page not in (None, '') else ''}\")\n",
        "    return resp\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}