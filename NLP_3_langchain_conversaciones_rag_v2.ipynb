{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gforconi/UTNIA2025/blob/main/NLP_3_langchain_conversaciones_rag_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fe6774",
      "metadata": {
        "id": "66fe6774"
      },
      "source": [
        "\n",
        "# LangChain paso a paso: conversación, memoria y RAG\n",
        "\n",
        "Este notebook muestra una **evolución progresiva** del uso de LangChain en tres etapas:\n",
        "\n",
        "1. **Conversación simple**: un ejemplo básico con un LLM.\n",
        "2. **Conversación con memoria (BufferMemory)**: extendemos el ejemplo para mantener historial.\n",
        "3. **Conversación con RAG**: cargamos documentos, creamos un índice vectorial (Chroma) y consultamos con **conversational RAG**.\n",
        "\n",
        "> **Requisitos**: Python 3.10+ y una clave de API de OpenAI en la variable de entorno `OPENAI_API_KEY`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304baff3",
      "metadata": {
        "id": "304baff3"
      },
      "source": [
        "## 0) Instalación y configuración"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b0855c",
      "metadata": {
        "id": "a5b0855c"
      },
      "source": [
        "\n",
        "### (Opcional) Usar modelo local con Ollama en lugar de OpenAI\n",
        "Si querés correr **local**, activá `USE_OLLAMA = True` y asegurate de tener `ollama` corriendo en `http://localhost:11434`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15ada22",
      "metadata": {
        "id": "a15ada22"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Selector de proveedor LLM: OpenAI (default) u Ollama (local)\n",
        "USE_OLLAMA = False  # ← cambiá a True para usar Ollama local\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "try:\n",
        "    if USE_OLLAMA:\n",
        "        from langchain_ollama import ChatOllama\n",
        "        llm_factory = lambda: ChatOllama(model=\"llama3\", temperature=0.1)\n",
        "    else:\n",
        "        llm_factory = lambda: ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
        "    print(\"Proveedor listo:\", \"Ollama\" if USE_OLLAMA else \"OpenAI\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Error al configurar el proveedor LLM. ¿Instalaste 'langchain-ollama' si USE_OLLAMA=True?\") from e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f64b922",
      "metadata": {
        "id": "5f64b922",
        "outputId": "4cbf6839-bc26-45d6-990c-94d4da810193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting langsmith>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.4.23-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
            "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.13.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
            "  Downloading openai-1.104.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from tiktoken) (2025.7.34)\n",
            "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting sniffio (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>4 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai)\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
            "  Using cached pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Using cached httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from langchain-community) (1.26.4)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (73 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
            "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.7 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-macosx_13_0_universal2.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from chromadb) (0.21.4)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb)\n",
            "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting importlib-resources (from chromadb)\n",
            "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting grpcio>=1.58.0 (from chromadb)\n",
            "  Downloading grpcio-1.74.0-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
            "Collecting typer>=0.9.0 (from chromadb)\n",
            "  Downloading typer-0.17.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (14 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.11.3-cp312-cp312-macosx_15_0_arm64.whl.metadata (41 kB)\n",
            "Collecting rich>=10.11.0 (from chromadb)\n",
            "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting jsonschema>=4.19.0 (from chromadb)\n",
            "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
            "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
            "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb)\n",
            "  Downloading rpds_py-0.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
            "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.24.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.1 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: sympy in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: filelock in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.8)\n",
            "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
            "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/geronimoforconi/Downloads/desarrollos/IA-UTN/.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
            "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
            "Downloading sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
            "Using cached tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (996 kB)\n",
            "Downloading openai-1.104.2-py3-none-any.whl (928 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m928.2/928.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Using cached jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl (320 kB)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl (469 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
            "Using cached pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl (89 kB)\n",
            "Downloading chromadb-1.0.20-cp39-abi3-macosx_11_0_arm64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
            "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl (46 kB)\n",
            "Downloading grpcio-1.74.0-cp312-cp312-macosx_11_0_universal2.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
            "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading langsmith-0.4.23-py3-none-any.whl (378 kB)\n",
            "Downloading mmh3-5.2.0-cp312-cp312-macosx_11_0_arm64.whl (40 kB)\n",
            "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "Downloading onnxruntime-1.22.1-cp312-cp312-macosx_13_0_universal2.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
            "Downloading orjson-3.11.3-cp312-cp312-macosx_15_0_arm64.whl (127 kB)\n",
            "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
            "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-macosx_11_0_arm64.whl (31 kB)\n",
            "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
            "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading rpds_py-0.27.1-cp312-cp312-macosx_11_0_arm64.whl (345 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading typer-0.17.3-py3-none-any.whl (46 kB)\n",
            "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-macosx_11_0_arm64.whl (104 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-macosx_10_13_universal2.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-macosx_11_0_arm64.whl (394 kB)\n",
            "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading zstandard-0.24.0-cp312-cp312-macosx_11_0_arm64.whl (640 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.3/640.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=a71b282885ea84974469df0be3a7d2fc8a5979e6eaab0d931bba7050f777ea3a\n",
            "  Stored in directory: /Users/geronimoforconi/Library/Caches/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, flatbuffers, durationpy, zstandard, zipp, websockets, websocket-client, uvloop, typing-inspection, tenacity, SQLAlchemy, sniffio, shellingham, rpds-py, requests, python-dotenv, pyproject_hooks, pypdf, pydantic-core, pybase64, pyasn1, protobuf, propcache, overrides, orjson, oauthlib, mypy-extensions, multidict, mmh3, mdurl, marshmallow, jsonpointer, jiter, importlib-resources, humanfriendly, httpx-sse, httptools, h11, grpcio, frozenlist, distro, click, cachetools, bcrypt, backoff, attrs, annotated-types, aiohappyeyeballs, yarl, uvicorn, typing-inspect, tiktoken, rsa, requests-toolbelt, requests-oauthlib, referencing, pydantic, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, jsonpatch, importlib-metadata, httpcore, googleapis-common-protos, coloredlogs, build, anyio, aiosignal, watchfiles, rich, pydantic-settings, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, jsonschema-specifications, httpx, google-auth, dataclasses-json, aiohttp, typer, opentelemetry-semantic-conventions, openai, langsmith, kubernetes, jsonschema, opentelemetry-sdk, langchain-core, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, langchain, chromadb, langchain-community\n",
            "\u001b[2K  Attempting uninstall: requests━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/94\u001b[0m [SQLAlchemy]lient]\n",
            "\u001b[2K    Found existing installation: requests 2.32.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/94\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K    Uninstalling requests-2.32.4:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/94\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K      Successfully uninstalled requests-2.32.4━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/94\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94/94\u001b[0m [langchain-community]ommunity]ore]dk]protos]\n",
            "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.10.0 attrs-25.3.0 backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 cachetools-5.5.2 chromadb-1.0.20 click-8.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 distro-1.9.0 durationpy-0.10 flatbuffers-25.2.10 frozenlist-1.7.0 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.74.0 h11-0.16.0 httpcore-1.0.9 httptools-0.6.4 httpx-0.28.1 httpx-sse-0.4.1 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 kubernetes-33.1.0 langchain-0.3.27 langchain-community-0.3.29 langchain-core-0.3.75 langchain-openai-0.3.32 langchain-text-splitters-0.3.11 langsmith-0.4.23 markdown-it-py-4.0.0 marshmallow-3.26.1 mdurl-0.1.2 mmh3-5.2.0 multidict-6.6.4 mypy-extensions-1.1.0 oauthlib-3.3.1 onnxruntime-1.22.1 openai-1.104.2 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 orjson-3.11.3 overrides-7.7.0 posthog-5.4.0 propcache-0.3.2 protobuf-6.32.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 pypdf-6.0.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.1 referencing-0.36.2 requests-2.32.5 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-14.1.0 rpds-py-0.27.1 rsa-4.9.1 shellingham-1.5.4 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.11.0 typer-0.17.3 typing-inspect-0.9.0 typing-inspection-0.4.1 uvicorn-0.35.0 uvloop-0.21.0 watchfiles-1.1.0 websocket-client-1.8.0 websockets-15.0.1 yarl-1.20.1 zipp-3.23.0 zstandard-0.24.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ✳️ Instala dependencias (ejecuta esta celda)\n",
        "# Sugerencia: usa un entorno virtual (venv/conda) antes de instalar.\n",
        "!pip install -U langchain langchain-openai langchain-community langchain-text-splitters chromadb pypdf tiktoken\n",
        "# Opcional: FAISS en lugar de Chroma\n",
        "# !pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4dab15",
      "metadata": {
        "id": "9e4dab15",
        "outputId": "356577d4-005d-4d2e-81a7-2ce55b395643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OPENAI_API_KEY detectada.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 🔐 Configura tu clave de OpenAI\n",
        "import os\n",
        "\n",
        "# Opción A (recomendada): exporta la variable en tu sistema antes de abrir el notebook:\n",
        "#   Linux/Mac: export OPENAI_API_KEY=\"tu_api_key\"\n",
        "#   Windows (PowerShell): setx OPENAI_API_KEY \"tu_api_key\"\n",
        "#\n",
        "# Opción B: establece la clave directamente aquí (no recomendado para producción):\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key\"\n",
        "\n",
        "# Opción B\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key_aqui\"\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"⚠️ No se encontró OPENAI_API_KEY en el entorno. Configúrala antes de continuar.\")\n",
        "else:\n",
        "    print(\"✅ OPENAI_API_KEY detectada.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe046c5",
      "metadata": {
        "id": "4fe046c5"
      },
      "source": [
        "\n",
        "## 1) Conversación simple con LangChain\n",
        "\n",
        "En esta primera sección creamos una mini **pipeline** con:\n",
        "- `ChatPromptTemplate` → define el prompt.\n",
        "- `ChatOpenAI` → el modelo de chat.\n",
        "- `StrOutputParser` → convierte la respuesta a `str` para imprimir fácilmente.\n",
        "\n",
        "La **LCEL (LangChain Expression Language)** permite conectar componentes con el operador `|`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2460310f",
      "metadata": {
        "id": "2460310f",
        "outputId": "4283019c-f1ff-4536-ead7-76ae6419cc59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, soy un asistente virtual diseñado para ayudarte con información y responder a tus preguntas. Puedo ofrecerte datos, consejos, explicaciones sobre diversos temas y más. ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Modelo de chat (elige el que prefieras; gpt-4o-mini es rápido y económico)\n",
        "# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Prompt de sistema + entrada del usuario\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente útil y conciso. Responde en español.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Construimos la cadena: prompt -> modelo -> parser de string\n",
        "simple_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# 👇 Prueba rápida\n",
        "respuesta = simple_chain.invoke({\"input\": \"Hola, ¿quién eres y qué puedes hacer?\"})\n",
        "print(respuesta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4e7bc0",
      "metadata": {
        "id": "2a4e7bc0"
      },
      "source": [
        "\n",
        "**Qué observar**  \n",
        "- `invoke(...)` ejecuta la cadena de principio a fin.  \n",
        "- Cambia `temperature` para controlar la creatividad.  \n",
        "- Modifica el texto de `system` para ajustar el \"rol\" del asistente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd60868d",
      "metadata": {
        "id": "dd60868d"
      },
      "source": [
        "### Temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdf633b6",
      "metadata": {
        "id": "bdf633b6"
      },
      "source": [
        "Con `temperature = 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "450c8e7a",
      "metadata": {
        "id": "450c8e7a",
        "outputId": "228f73a8-6b30-4c64-87ce-3e7c754f2c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo información en tiempo real, pero puedo ayudarte con conceptos, temas y preguntas sobre la clase de Inteligencia Artificial. ¿Qué aspecto específico te gustaría conocer?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6608729c",
      "metadata": {
        "id": "6608729c",
        "outputId": "ecc437c6-7de5-4fa7-e635-7c2ba3218bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a información en tiempo real, pero generalmente las clases de Inteligencia Artificial se centran en temas como el aprendizaje automático, redes neuronales, procesamiento de lenguaje natural y ética en IA. Si quieres saber sobre un tema específico, dime y con gusto te ayudo.\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c806b41c",
      "metadata": {
        "id": "c806b41c"
      },
      "source": [
        "Con `temperature = 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e57257",
      "metadata": {
        "id": "63e57257",
        "outputId": "08285a54-7a24-4040-a627-001e9fc14e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a información en tiempo real, pero generalmente las clases de Inteligencia Artificial suelen ser muy interesantes y abarcan temas como aprendizaje automático, redes neuronales y procesamiento de lenguaje natural. ¿Te gustaría saber algo específico sobre el tema?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "910ac6b0",
      "metadata": {
        "id": "910ac6b0",
        "outputId": "2d795915-4346-4835-eca5-78c6c2046a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No tengo acceso a información en tiempo real, pero generalmente las clases de Inteligencia Artificial suelen ser muy interesantes y abarcan temas como aprendizaje automático, redes neuronales y procesamiento de lenguaje natural. ¿Te gustaría saber algo específico sobre el tema?\n"
          ]
        }
      ],
      "source": [
        "respuesta = simple_chain.invoke({\"input\": \"Que tal esta la clase de Inteligencia Artificial hoy?\"})\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a86c9bb",
      "metadata": {
        "id": "8a86c9bb"
      },
      "source": [
        "`temperature = 0.0`\n",
        "\n",
        "El modelo es **determinista**: casi siempre devuelve la misma respuesta para la misma entrada.\n",
        "Útil cuando buscás **precisión** y consistencia (ej. generar SQL, código, respuestas factuales).\n",
        "\n",
        "**Valores bajos (0.1 – 0.3)**\n",
        "\n",
        "El modelo varía un poco sus respuestas, pero sigue siendo bastante confiable.\n",
        "Ideal para **chatbots serios**, resúmenes o asistencia técnica.\n",
        "\n",
        "**Valores medios (0.5 – 0.7)**\n",
        "\n",
        "Las respuestas son más variadas, con sinónimos, estilos distintos o explicaciones alternativas.\n",
        "Útil en contextos de **escritura creativa** o **brainstorming**.\n",
        "\n",
        "**Valores altos (0.8 – 1.0 o más)**\n",
        "\n",
        "El modelo se vuelve **muy creativo y arriesgado**, pudiendo inventar cosas o desviarse del tema.\n",
        "Puede servir para poesía, storytelling, generación de ideas “locas”.\n",
        "Pero en tareas técnicas, aumenta la probabilidad de errores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227881f0",
      "metadata": {
        "id": "227881f0"
      },
      "source": [
        "\n",
        "## 2) Conversación con memoria (ConversationBufferMemory)\n",
        "\n",
        "Ahora extendemos el ejemplo para **recordar el historial** de la charla usando `ConversationBufferMemory` y `ConversationChain`.\n",
        "\n",
        "> **Nota**: `ConversationBufferMemory` está **deprecado** en LangChain (se mantendrá hasta la versión 1.0). Sigue siendo útil para aprender; para proyectos nuevos, LangChain recomienda usar `RunnableWithMessageHistory` o enfoques basados en **LCEL** (los verás en la sección 3).\n",
        "\n",
        "**Objetivo**: que el bot entienda referencias como “eso”, “lo anterior”, “ella/él”, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af70c569",
      "metadata": {
        "id": "af70c569",
        "outputId": "a51fb383-7711-4ee0-80c5-f070068b7b99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ck/qx4468vx3sx6bf4nt0smfg2c0000gn/T/ipykernel_86485/261983815.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n",
            "/var/folders/ck/qx4468vx3sx6bf4nt0smfg2c0000gn/T/ipykernel_86485/261983815.py:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usuario: Hola, soy Ana y me gusta Python.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hola, soy Ana y me gusta Python.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Asistente: ¡Hola, Ana! Encantado de conocerte. Python es un lenguaje de programación muy versátil y popular. ¿Qué es lo que más te gusta de Python? ¿Estás trabajando en algún proyecto en particular o aprendiendo algo nuevo?\n",
            "\n",
            "Usuario: ¿Qué lenguaje dije que me gustaba?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hola, soy Ana y me gusta Python.', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Hola, Ana! Encantado de conocerte. Python es un lenguaje de programación muy versátil y popular. ¿Qué es lo que más te gusta de Python? ¿Estás trabajando en algún proyecto en particular o aprendiendo algo nuevo?', additional_kwargs={}, response_metadata={})]\n",
            "Human: ¿Qué lenguaje dije que me gustaba?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Asistente: Dijiste que te gusta Python. Es un lenguaje muy popular y versátil. ¿Qué es lo que más te atrae de él?\n",
            "\n",
            "--- Memoria acumulada ---\n",
            "HumanMessage → Hola, soy Ana y me gusta Python.\n",
            "AIMessage → ¡Hola, Ana! Encantado de conocerte. Python es un lenguaje de programación muy versátil y popular. ¿Qué es lo que más te gusta de Python? ¿Estás trabajando en algún proyecto en particular o aprendiendo algo nuevo?\n",
            "HumanMessage → ¿Qué lenguaje dije que me gustaba?\n",
            "AIMessage → Dijiste que te gusta Python. Es un lenguaje muy popular y versátil. ¿Qué es lo que más te atrae de él?\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "\n",
        "# Guardamos TODO el historial de mensajes en memoria\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# La ConversationChain une LLM + memoria\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "# 🗣️ Diálogo de ejemplo\n",
        "print(\"Usuario: Hola, soy Ana y me gusta Python.\")\n",
        "out1 = conversation.invoke(\"Hola, soy Ana y me gusta Python.\")\n",
        "print(\"Asistente:\", out1[\"response\"])\n",
        "\n",
        "print(\"\\nUsuario: ¿Qué lenguaje dije que me gustaba?\")\n",
        "out2 = conversation.invoke(\"¿Qué lenguaje dije que me gustaba?\")\n",
        "print(\"Asistente:\", out2[\"response\"])\n",
        "\n",
        "# Puedes inspeccionar el buffer de memoria\n",
        "print(\"\\n--- Memoria acumulada ---\")\n",
        "for m in memory.chat_memory.messages:\n",
        "    print(type(m).__name__, \"→\", m.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "360e5be2",
      "metadata": {
        "id": "360e5be2"
      },
      "source": [
        "\n",
        "**Qué observar**  \n",
        "- La memoria **no** es persistente entre ejecuciones del notebook; vive en RAM.  \n",
        "- Para memorias largas o persistentes, considera `ConversationSummaryMemory` o guardar/recargar historiales.  \n",
        "- En la sección 3 usamos la alternativa moderna con `RunnableWithMessageHistory`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6be6bb9",
      "metadata": {
        "id": "c6be6bb9"
      },
      "source": [
        "`ConversationBufferMemory` está deprecado en LangChain (se mantiene hasta langchain==1.0.0), así que lo muestro para aprender pero en la sección 3 uso el enfoque moderno con RunnableWithMessageHistory y history-aware retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb5b344",
      "metadata": {
        "id": "8eb5b344"
      },
      "source": [
        "\n",
        "## 3) Conversación con RAG (consultas a una base vectorial)\n",
        "\n",
        "En esta sección implementamos un **RAG conversacional**:\n",
        "1. Cargamos documentos locales (carpeta `./data`).\n",
        "2. Los partimos en fragmentos.\n",
        "3. Creamos embeddings y un **índice vectorial** con **Chroma**.\n",
        "4. Usamos un **retriever** que busca los pasajes más relevantes.\n",
        "5. Armamos una cadena **history-aware** (consciente del historial) para contextualizar preguntas de seguimiento.\n",
        "6. Mantenemos el historial con `RunnableWithMessageHistory`.\n",
        "\n",
        "> Si no tienes documentos, crearemos **automáticamente** algunos de ejemplo en `./data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e647e5",
      "metadata": {
        "id": "03e647e5",
        "outputId": "fc8e00d1-7c75-4036-cdfc-49bead32e66d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 3077.26it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos cargados: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.1) Carga de documentos\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Si la carpeta está vacía, creamos documentos de ejemplo\n",
        "if not any(data_dir.iterdir()):\n",
        "    (data_dir / \"intro_rag.txt\").write_text(\n",
        "        \"RAG (Retrieval-Augmented Generation) combina recuperación de contexto con generación. \"\n",
        "        \"Permite responder con información actualizada sin reentrenar el modelo.\"\n",
        "    )\n",
        "    (data_dir / \"langchain_nociones.md\").write_text(\n",
        "        \"# Nociones de LangChain\\n\"\n",
        "        \"- LCEL permite encadenar componentes con el operador |.\\n\"\n",
        "        \"- Los retrievers devuelven documentos relevantes.\\n\"\n",
        "        \"- Los vector stores como Chroma y FAISS almacenan embeddings.\"\n",
        "    )\n",
        "\n",
        "# Cargamos .txt y .md con TextLoader, y .pdf con PyPDFLoader\n",
        "loaders = []\n",
        "# Text/Markdown\n",
        "loaders.append(DirectoryLoader(str(data_dir), glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True))\n",
        "loaders.append(DirectoryLoader(str(data_dir), glob=\"**/*.md\", loader_cls=TextLoader, show_progress=True))\n",
        "# PDF\n",
        "for pdf in data_dir.glob(\"**/*.pdf\"):\n",
        "    loaders.append(PyPDFLoader(str(pdf)))\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "print(f\"Documentos cargados: {len(docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4565317",
      "metadata": {
        "id": "a4565317",
        "outputId": "4bedf236-441c-4110-ec29-4bcdc65d4d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks creados: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.2) Particionado de documentos en chunks\n",
        "# En versiones recientes, los 'splitters' residen en 'langchain_text_splitters'\n",
        "try:\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "except ImportError:\n",
        "    # Fallback si usas una versión anterior\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800, # Cada fragmento tendrá como máximo 800 caracteres.\n",
        "    chunk_overlap=120, # Cada fragmento se solapa con el siguiente en 120 caracteres.\n",
        "    add_start_index=True # Añade el índice de inicio del chunk en el documento original (útil para referencias).\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Chunks creados: {len(splits)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1af6332",
      "metadata": {
        "id": "b1af6332"
      },
      "source": [
        "`chunk_size=800`\n",
        "\n",
        "Cada fragmento tendrá como máximo 800 caracteres.\n",
        "Esto ayuda a que el LLM no reciba textos demasiado largos en una sola pasada.\n",
        "\n",
        "`chunk_overlap=120`\n",
        "\n",
        "Cada fragmento se solapa con el siguiente en 120 caracteres.\n",
        "Sirve para que no se “corte” información importante justo en el límite entre dos chunks.\n",
        "Ejemplo: si un párrafo empieza al final de un chunk, el solapamiento asegura que también aparezca al inicio del siguiente.\n",
        "\n",
        "`add_start_index=True`\n",
        "\n",
        "Agrega un índice de inicio (posición dentro del texto original) en la metadata de cada fragmento.\n",
        "Eso es útil si después querés rastrear en qué parte exacta del documento estaba el chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989da1a4",
      "metadata": {
        "id": "989da1a4",
        "outputId": "09c4b366-28c6-4325-d247-f3f922f0480d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store listo ✓\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.3) Embeddings + Vector Store (Chroma)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "persist_dir = \"chroma_db\"\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_dir  # para que puedas reusar el índice\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "print(\"Vector store listo ✓\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a76575",
      "metadata": {
        "id": "b6a76575",
        "outputId": "0b668c00-901f-4d98-cadc-44788d8be6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cadenas de RAG construidas ✓\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.4) Cadena de RAG con historial (enfoque recomendado)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "llm = llm_factory()\n",
        "\n",
        "# Prompt que reescribe la consulta considerando el historial (si lo hay)\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Reescribe la consulta del usuario para búsqueda, teniendo en cuenta el historial.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    prompt=contextualize_q_prompt\n",
        ")\n",
        "\n",
        "# Prompt final para responder con los documentos recuperados\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Eres un asistente experto. Usa EXCLUSIVAMENTE el 'contexto' proporcionado para responder en español. \"\n",
        "     \"Si la respuesta no está en el contexto, di claramente que no aparece.\\n\\n\"\n",
        "     \"===== CONTEXTO =====\\n{context}\\n=====================\\n\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, combine_docs_chain)\n",
        "\n",
        "print(\"Cadenas de RAG construidas ✓\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480fe48a",
      "metadata": {
        "id": "480fe48a",
        "outputId": "57f39d1a-0744-4dac-d5f1-dd980171c287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pregunta 1: ¿Qué es RAG?\n",
            "→ RAG (Retrieval-Augmented Generation) combina recuperación de contexto con generación. Permite responder con información actualizada sin reentrenar el modelo.\n",
            "\n",
            "Pregunta 2: ¿Y cómo se relaciona con LangChain? (nota: usa un pronombre)\n",
            "→ No aparece.\n",
            "\n",
            "Pregunta 3: ¿Dónde mencionaste los retrievers?\n",
            "→ Los retrievers se mencionan en el contexto como aquellos que devuelven documentos relevantes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3.5) Añadir memoria de conversación con RunnableWithMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory  # almacenamiento simple en memoria\n",
        "\n",
        "# Diccionario {session_id: ChatMessageHistory}\n",
        "_session_store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in _session_store:\n",
        "        _session_store[session_id] = ChatMessageHistory()\n",
        "    return _session_store[session_id]\n",
        "\n",
        "rag_with_history = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",  # la clave por defecto para create_retrieval_chain\n",
        ")\n",
        "\n",
        "# Demostración de diálogo multi-turno con el mismo session_id\n",
        "config = {\"configurable\": {\"session_id\": \"demo\"}}\n",
        "\n",
        "def conversar(pregunta: str):\n",
        "    resp = rag_with_history.invoke({\"input\": pregunta}, config=config)\n",
        "    print(\"→\", resp[\"answer\"])\n",
        "    # Si quieres inspeccionar qué documentos usó:\n",
        "    # for i, d in enumerate(resp.get(\"context\", []), 1):\n",
        "    #     print(f\"[{i}] {d.metadata.get('source', 'doc')}:{d.metadata.get('page', '')}\")\n",
        "\n",
        "print(\"Pregunta 1: ¿Qué es RAG?\")\n",
        "conversar(\"¿Qué es RAG?\")\n",
        "\n",
        "print(\"\\nPregunta 2: ¿Y cómo se relaciona con LangChain? (nota: usa un pronombre)\")\n",
        "conversar(\"¿Y cómo se relaciona con LangChain?\")\n",
        "\n",
        "print(\"\\nPregunta 3: ¿Dónde mencionaste los retrievers?\")\n",
        "conversar(\"¿Dónde mencionaste los retrievers?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36068732",
      "metadata": {
        "id": "36068732"
      },
      "source": [
        "### Consultas la base de vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5206fcb",
      "metadata": {
        "id": "a5206fcb",
        "outputId": "161bfcde-8115-44dc-f65e-f3fe05fadeab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8152427673339844 data/intro_rag.txt\n",
            "1.3764668703079224 data/langchain_nociones.md\n"
          ]
        }
      ],
      "source": [
        "query = \"¿Qué es RAG?\"\n",
        "pairs = vectordb.similarity_search_with_score(query, k=4)\n",
        "for doc, score in pairs:\n",
        "    print(score, doc.metadata.get(\"source\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bd742f",
      "metadata": {
        "id": "70bd742f",
        "outputId": "936423b8-222f-4007-fd01-9a1e5936cda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] data/intro_rag.txt  pág: None\n",
            "RAG (Retrieval-Augmented Generation) combina recuperación de contexto con generación. Permite responder con información actualizada sin reentrenar el modelo. ...\n",
            "\n",
            "[2] data/langchain_nociones.md  pág: None\n",
            "# Nociones de LangChain\n",
            "- LCEL permite encadenar componentes con el operador |.\n",
            "- Los retrievers devuelven documentos relevantes.\n",
            "- Los vector stores como Chroma y FAISS almacenan embeddings. ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"¿Qué es RAG?\"\n",
        "docs = retriever.invoke(query)  # equivalente a retriever.get_relevant_documents(query)\n",
        "\n",
        "for i, d in enumerate(docs, 1):\n",
        "    print(f\"[{i}] {d.metadata.get('source', 'doc')}  pág: {d.metadata.get('page')}\")\n",
        "    print(d.page_content[:300], \"...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3774931e",
      "metadata": {
        "id": "3774931e"
      },
      "source": [
        "\n",
        "### Notas y buenas prácticas\n",
        "- Si ves advertencias de *deprecations*, revisa las guías de migración de LangChain y actualiza imports.\n",
        "- Para **persistir** historiales entre sesiones, puedes reemplazar `ChatMessageHistory` por alternativas como `FileChatMessageHistory` o una base de datos.\n",
        "- **FAISS** es otra base vectorial popular (`faiss-cpu`). El código cambia poco: `from langchain_community.vectorstores import FAISS` y `FAISS.from_documents(...)`.\n",
        "- Ajusta `chunk_size`/`chunk_overlap` según el tamaño de tus documentos y el contexto del modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1e6f9d",
      "metadata": {
        "id": "ae1e6f9d"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Créditos y recursos\n",
        "- Documentación de **ChatOpenAI** y `langchain-openai` (instalación y uso).\n",
        "- `ConversationBufferMemory` (estado: deprecado, pero útil para aprender).\n",
        "- RAG moderno con `create_history_aware_retriever` y `create_retrieval_chain`.\n",
        "- `create_stuff_documents_chain` para combinar documentos en el prompt.\n",
        "\n",
        "> Revisa las guías oficiales para ejemplos actualizados, cambios en APIs y mejores prácticas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81b249db",
      "metadata": {
        "id": "81b249db"
      },
      "source": [
        "### Función de ayuda: `preguntar()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "896374e7",
      "metadata": {
        "id": "896374e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preguntar(texto: str, session_id: str = \"demo\", mostrar_fuentes: bool = True):\n",
        "    if 'rag_with_history' in globals():\n",
        "        config = {\"configurable\": {\"session_id\": session_id}}\n",
        "        resp = rag_with_history.invoke({\"input\": texto}, config=config)\n",
        "    elif 'rag_chain' in globals():\n",
        "        resp = rag_chain.invoke({\"input\": texto, \"chat_history\": []})\n",
        "    else:\n",
        "        raise RuntimeError(\"No encontré 'rag_with_history' ni 'rag_chain'. Ejecutá las celdas de la sección 3.4 y 3.5.\")\n",
        "\n",
        "    print(resp.get(\"answer\") or resp)\n",
        "    if mostrar_fuentes and isinstance(resp, dict) and \"context\" in resp:\n",
        "        print(\"\\nFuentes:\")\n",
        "        for i, d in enumerate(resp[\"context\"], 1):\n",
        "            src = d.metadata.get(\"source\", \"doc\")\n",
        "            page = d.metadata.get(\"page\", \"\")\n",
        "            print(f\"[{i}] {src} {('pág ' + str(page)) if page not in (None, '') else ''}\")\n",
        "    return resp\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}